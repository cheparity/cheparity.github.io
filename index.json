[{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/","section":"Niyuta's Blog","summary":"","title":"Niyuta's Blog","type":"page"},{"content":"","date":"21 October 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"轨迹规划 (Trajectory Plan) | 知乎\n轨迹规划的种类 # Joint Space 下的轨迹规划 # 定义末端执行器坐标系 ${T}$ 相对于世界坐标系 Global ${G}$ 的 initial, via, \u0026amp; final points, ${}^G T_i$ $i=1$ initial $i=2 \\sim N-1$ via points $i=N+1$ final 将 ${}^G T_i$ 以 $6$ 个参数方式（包含 3 个移动自由度和 3 个转动自由度）来表达 $$ ^G X_T = \\begin{bmatrix} ^G P_{T\\; org} \\\\ ROT({}^G \\hat{K}_T, \\theta) \\end{bmatrix} $$ Explaination 位置分量（3 个参数）： $^G P_{T; org}$ - $^G P_{T; org}$：这是从基坐标系 ${G}$ 的原点指向末端执行器坐标系 ${T}$ 原点的位置向量 (Position Vector)。 - 它通常是一个 $3 \\times 1$ 的向量，代表末端执行器在 ${G}$ 坐标系中的 $(x, y, z)$ 坐标。 姿态分量（3 个参数）： $ROT({}^G \\hat{K}_T, \\theta)$ - $ROT(\\dots)$：表示旋转 (Rotation) 或方向 (Orientation)。 - ${}^G \\hat{K}_T$ 和 $\\theta$：这表示使用轴 - 角 (Axis-Angle) 的方式来描述末端执行器相对于基坐标系 ${G}$ 的方向。 ${}^G \\hat{K}_T$ 是旋转的单位轴向量，在 ${G}$ 中表示。由于 $\\hat{K}_T$ 是单位向量（长度为 1），它只需要 2 个独立参数来定义方向。比如 $XY$ 平面的 45 度方向 $$ \u003e \\hat{K}_T = \\begin{bmatrix} \\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\\\ 0 \\end{bmatrix} \u003e $$ $\\theta$ 是绕该轴旋转的角度。 将轴 - 角参数组合起来，可以用 3 个独立参数来表示一个三维旋转（例如，通过欧拉角 RPY 或轴 - 角向量 $\\theta\\hat{K}_T$ 的三个分量）。 对所有手臂末端点状态规划 smooth trajectories（其实就是插值） 将规划好手臂末端点状态的轨迹点经过 I.K.转换为关节 joint 状态：$^G X_T \\to \\Theta_t$ 检查 joint 状态在 Joint-space 下轨迹的可行性 Cartesian-space 笛卡尔空间 # 确定轨迹的起点 (Initial Point) - 途径点 (Via Point) - 终点 (Finial Point) 设计一条曲线将 Cartesian-space 所有点都平滑的连接起来。 使用 IK 计算出这条曲线在 Joint-space 的曲线。 检查 Joint-space 的曲线是否平滑。 对比 Cartesian \u0026amp; Joint space 的区别显而易见：进行规划的主体不同。Cartesian space 中直接对末端执行器进行数学规划，Joint space 则需要先把末端执行器的 space IK 成为 Joint space，再进行数学规划。\n轨迹规划算法 # 三次/五次多项式样条插值法 # 分 Cubic Polynomials Spline Interpolation 三次多项式方法和 Quintic Polynomials Spline Interpolation 五次样条插值法。这俩核心思想是一样的。\n在每一段轨迹 $P_{i} \\to P_{i+1}$ 上，用一个三次多项式来描述位置和时间的关系：\n$$ \\theta(t)=a_{0}+a_{1}t+a_{2}t^{2}+a_{3}t^{3}, t \\in [t_{i},t_{i+1}] $$ 为什么是三次多项式？ 因为我们在一段轨迹中，通常希望同时控制\n初始位置 $\\theta(t_{i})$ 终点位置 $\\theta(t_{i+1})$ 初始速度 $\\dot{\\theta}(t_i)$ 终点速度 $\\dot{\\theta}(t_{i+1})$ 三次函数正好有四个未知数，对应了上面能列出来的四个方程： $$ \u003e \\begin{cases} \u003e \\theta(t_i) = a_0 + a_1 t_i + a_2 t_i^2 + a_3 t_i^3 \\\\ \u003e \\dot{\\theta}(t_i) = a_1 + 2a_2 t_i + 3a_3 t_i^2 \\\\ \u003e \\theta(t_{i+1}) = a_0 + a_1 t_{i+1} + a_2 t_{i+1}^2 + a_3 t_{i+1}^3 \\\\ \u003e \\dot{\\theta}(t_{i+1}) = a_1 + 2a_2 t_{i+1} + 3a_3 t_{i+1}^2 \u003e \\end{cases} \\tag{1} \u003e $$ 如果再加上加速度连续性获得的两个方程，就可以是五次多项式方法。 $(1)$ 式是核心。为了满足每一段的轨迹相交且光滑，所以对任意两个相邻的多项式曲线而言，上一段的终点和下一段的起点是重合的，还需要满足此点处两条曲线的一次微分和二次微分相等。\n多段轨迹的连续性要求 # 对于一段由 $N$ 个路点 $P_0, P_1, \\dots, P_N$ 组成的轨迹，我们需要使用 $N$ 个三次多项式 $\\theta_i(t)$ 来连接相邻的路点 $P_i \\to P_{i+1}$。\n为了确保整个轨迹 $\\theta(t)$ 运动平稳，我们需要在**路点（连接点）**处满足一定的连续性条件。\n1. 位置连续性 (C0) # 在每一个路点 $P_i$ 处，前一段轨迹的终点位置必须等于后一段轨迹的起点位置。这是最基本的几何连续性。\n$$ \\theta_{i-1}(t_i) = \\theta_i(t_i) = P_i $$ 2. 速度连续性 (C1) # 速度的连续性是三次多项式方法最核心的要求，它确保了机械臂在经过路点时不会突然变速。\n$$ \\dot{\\theta}_{i-1}(t_i) = \\dot{\\theta}_i(t_i) $$ 3. 加速度连续性 (C2) # 更高阶的连续性要求是：\n$$ \\ddot{\\theta}_{i-1}(t_i) = \\ddot{\\theta}_i(t_i) $$然而，对于标准的三次多项式方法，我们通常只满足 C1 连续性（即速度连续），而无法保证 C2 连续性（即加速度连续）。C2 连续性是五次多项式方法。\n三次多项式的局限性 每一段三次多项式 $\\theta_i(t)$ 只有 4 个系数 ($a_0$ 到 $a_3$)，因此在一段轨迹中只能满足 4 个边界条件：\n$\\theta_i(t_i)$ 和 $\\dot{\\theta}_i(t_i)$\n$\\theta_i(t_{i+1})$ 和 $\\dot{\\theta}i(t{i+1})$\n即使在连接点 $t_i$ 处，我们强制要求：\n$\\dot{\\theta}_{i-1}(t_i) = \\dot{\\theta}_i(t_i)$（速度连续）\n加速度 $\\ddot{\\theta}(t)$ 在连接点是无法保证连续的。 加速度 $\\ddot{\\theta}(t)$ 是关于时间 $t$ 的线性函数，它的值由多项式系数决定，且通常会在路点处产生跳变。\n$$ \u003e \\ddot{\\theta}_i(t) = 2a_{2,i} + 6a_{3,i}t \u003e $$这意味着在整个轨迹的连接点处，机械臂会经历冲击 (Jerk)，需要更高级的五次多项式或样条曲线等方法来解决。\n速度规划：开放/封闭轨迹 # 开放式轨迹 (Open Trajectory) # 对于一条从起点 $P_0$ 到终点 $P_N$ 的开放式轨迹，我们需要确定 $N$ 个轨迹段的 $4N$ 个系数。我们有 $4N$ 个边界条件：\n起点/终点条件： $P_0$ 的 $\\theta(t_0), \\dot{\\theta}(t_0)$ 和 $P_N$ 的 $\\theta(t_N), \\dot{\\theta}(t_N)$（共 4 个已知条件）。 中间路点条件：对于 $N-1$ 个中间路点 $P_1, \\dots, P_{N-1}$，每个路点需要满足位置连续 ($\\theta_{i-1}(t_i) = \\theta_i(t_i)$) 和速度连续 ($\\dot{\\theta}_{i-1}(t_i) = \\dot{\\theta}_i(t_i)$)。（共 $2(N-1)$ 个约束）。 总边界条件数： $4 + 2(N-1) = 2N + 2$。\n缺失的条件： $4N - (2N+2) = 2N - 2$ 个自由度。\n如何补足自由度？ 我们需要为 $N-1$ 个中间路点指定速度 $\\dot{\\theta}(t_i)$。通常有以下方法：\n全部指定为零： 机械臂在每个路点处停止，路径清晰，但效率低（适用于“通过路点”的任务）。\n内插速度： 使用某种平滑的内插方法（例如，通过相邻路点和时间计算出近似的连续速度，如三次样条中的速度计算）来确定中间速度。\n封闭式轨迹 (Closed Trajectory) # 如果轨迹是首尾相接的 ($P_N = P_0$)，则还需要满足首尾相接点的连续性：\n$$ \\theta_{N-1}(t_N) = \\theta_0(t_0) $$$$ \\dot{\\theta}_{N-1}(t_N) = \\dot{\\theta}_0(t_0) $$这增加了额外的约束，常用于循环作业或周期性运动的规划。\nCode # Robotics Toolbox for Python (RTB) 可以用来求解。一般工业上都用五次多项式。\npip install roboticstoolbox-python numpy matplotlib import numpy as np import roboticstoolbox as rtb import matplotlib.pyplot as plt # --- 1. 定义轨迹参数 --- # 机械臂的自由度 (DOF) DOF = 6 # 起始关节角度 (q0, 位置 P0) [rad] # 假设机械臂从零位开始 q0 = np.zeros(DOF) # 终止关节角度 (qf, 位置 Pf) [rad] # 假设关节 1, 2, 3 移动到特定角度 qf = np.array([np.pi/4, np.pi/6, -np.pi/8, 0, 0, 0]) # 轨迹持续时间 (秒) T_total = 4.0 # 轨迹点数量 (用于绘图和控制的采样点) N = 101 # 生成时间向量 t = np.linspace(0, T_total, N) # --- 2. 使用 jtraj() 生成轨迹 --- # rtb.jtraj() 默认使用五次多项式 (quintic polynomial) # 默认的边界条件是： # 初始速度 qd0 = 0, 初始加速度 qdd0 = 0 # 终点速度 qdf = 0, 终点加速度 qddf = 0 # 这确保了轨迹是 C2 连续的。 # 注意: q0 和 qf 必须是 NumPy 数组或列表 traj = rtb.jtraj(q0, qf, t) # --- 3. 提取轨迹数据 --- q_traj = traj.q # 关节位置矩阵 (N x DOF) qd_traj = traj.qd # 关节速度矩阵 (N x DOF) qdd_traj = traj.qdd # 关节加速度矩阵 (N x DOF) # 打印检查 print(f\u0026#34;总轨迹点数量: {q_traj.shape[0]}\u0026#34;) print(f\u0026#34;起始位置 (q0): {q_traj[0, :3].round(4)}\u0026#34;) print(f\u0026#34;终点位置 (qf): {q_traj[-1, :3].round(4)}\u0026#34;) print(f\u0026#34;终点速度 (qd_f): {qd_traj[-1, :3].round(4)}\u0026#34;) print(f\u0026#34;终点加速度 (qdd_f): {qdd_traj[-1, :3].round(4)}\u0026#34;) # 终点速度和加速度都应接近于0 # --- 4. 轨迹可视化 (以第一个关节为例) --- joint_index = 0 # 查看第一个关节 (q1) 的运动 plt.figure(figsize=(12, 9)) # 1. 位置 plt.subplot(3, 1, 1) plt.plot(t, q_traj[:, joint_index]) plt.title(f\u0026#39;Joint {joint_index+1} Trajectory - Position $\\\\theta(t)$\u0026#39;) plt.ylabel(\u0026#39;Position (rad)\u0026#39;) plt.grid(True) # 2. 速度 plt.subplot(3, 1, 2) plt.plot(t, qd_traj[:, joint_index], color=\u0026#39;orange\u0026#39;) plt.title(f\u0026#39;Joint {joint_index+1} Trajectory - Velocity $\\\\dot{\\\\theta}(t)$\u0026#39;) plt.ylabel(\u0026#39;Velocity (rad/s)\u0026#39;) plt.grid(True) # 检查边界速度是否为零 plt.scatter([t[0], t[-1]], [qd_traj[0, joint_index], qd_traj[-1, joint_index]], color=\u0026#39;black\u0026#39;) # 3. 加速度 plt.subplot(3, 1, 3) plt.plot(t, qdd_traj[:, joint_index], color=\u0026#39;red\u0026#39;) plt.title(f\u0026#39;Joint {joint_index+1} Trajectory - Acceleration $\\\\ddot{\\\\theta}(t)$\u0026#39;) plt.xlabel(\u0026#39;Time (s)\u0026#39;) plt.ylabel(\u0026#39;Acceleration (rad/s²)\u0026#39;) plt.grid(True) # 检查边界加速度是否为零 plt.scatter([t[0], t[-1]], [qdd_traj[0, joint_index], qdd_traj[-1, joint_index]], color=\u0026#39;black\u0026#39;) plt.tight_layout() plt.show() ","date":"21 October 2025","externalUrl":null,"permalink":"/posts/ai/eai/robotics/%E8%BD%A8%E8%BF%B9%E8%A7%84%E5%88%92/","section":"Posts","summary":"","title":"轨迹规划","type":"posts"},{"content":" 机械臂正运动学 (Forward Kinematics, FK) # MDH (Modified Denavit-Hartenberg) 参数 # 通常把地面 ground 作为 i=0 的杆（link）。坐标系有可能不在实物上，你不能把实物当成抽象。\n元素 符号 描述 连杆长度 $a_{i-1}$ 沿 $X_{i-1}$ 轴测量，从 $Z_{i-1}$ 到 $Z_i$ 的距离。 连杆扭角 $\\alpha_{i-1}$ 绕 $X_{i-1}$ 轴测量，从 $Z_{i-1}$ 到 $Z_i$ 的角度。 关节距离 $d_{i}$ 沿 $Z_i$ 轴测量，从 $X_{i-1}$ 到 $X_i$ 的距离。 关节角 $\\theta_{i}$ 绕 $Z_i$ 轴测量，从 $X_{i-1}$ 到 $X_i$ 的角度。 MDH 坐标系建立规则。按 $Z-X-Y$ 的顺序去定：\n坐标系 $i$ 固连在连杆 $i$ 上。通常取地面作为 i=0 的杆。 $Z_i$ 轴沿关节 $i$ 的轴线方向。 $X_i$ 轴沿 $Z_i$ 和 $Z_{i+1}$ 的公垂线方向，或在公垂线为 0 时根据规则选择。 $Y_{i}$ 根据 $X_{i}$ 和 $Z_{i}$ 由右手定则确定。 坐标系 ${i}$ 建立在连杆 $i$ 的末端。 变换矩阵 # 从 $i-1 \\to i$ 的步骤：\n先对齐 $Z_{i-1} \\to Z_{i}$ 轴。绕 $X_{i-1}$ 轴转过两个转轴之间的差角 $\\alpha_{i-1}$（这一步旋转，两杆已经平行了）。 做平移，沿 $X_{i-1}$ 杆平移 $a_{i-1}$。此时两个坐标系 $Z$ 轴重合。 对齐 $X_{i-1} \\to X_{i}$ 轴，转过 $\\theta_{i}$。这不是为了对齐 $Z$ 轴，而是为了对齐公共法线 $X$ 轴。 沿 $Z_{i}$ 轴平移 $d_{i}$，对齐两个坐标系的原点。 Difference between Standard D-H and Modified D-H 特性 标准 D-H (SDH) 改进型 D-H (MDH) 坐标系 ${i}$ 固连位置 固连在连杆 $i$ 的输入端（即关节 $i$ 处）。 固连在连杆 $i$ 的输出端（即关节 $i+1$ 处）。 变换顺序 绕 $X_{i-1}$ 旋转 $\\alpha_{i-1}$ $\\to$ 沿 $X_{i-1}$ 平移 $a_{i-1}$ $\\to$ 绕 $Z_i$ 旋转 $\\theta_i$ $\\to$ 沿 $Z_i$ 平移 $d_i$ 绕 $Z_{i-1}$ 旋转 $\\theta_i$ $\\to$ 沿 $Z_{i-1}$ 平移 $d_i$ $\\to$ 绕 $X_i$ 旋转 $\\alpha_i$ $\\to$ 沿 $X_i$ 平移 $a_i$ 通过 MDH 参数，可以定义从连杆坐标系 ${i}$ 到 ${i-1}$ 的齐次变换矩阵 ${}{i}^{i-1}\\mathbf{T}$。即我们知道了右边，如何推到左边。下面这个式子右乘上 $P{i}$ 就能推导到左边 $P_{i-1}$ 了。\n$$ \\begin{align} {}{i}^{i-1}\\mathbf{T} \u0026amp;= \\text{Rot}(x, \\alpha{i-1}) \\cdot \\text{Trans}(x, a_{i-1}) \\cdot \\text{Rot}(z, \\theta_i) \\cdot \\text{Trans}(z, d_i) \\\n\u0026amp;=\\begin{pmatrix} \\cos\\theta_i \u0026amp; -\\sin\\theta_i \u0026amp; 0 \u0026amp; a_{i-1} \\ \\sin\\theta_i \\cos\\alpha_{i-1} \u0026amp; \\cos\\theta_i \\cos\\alpha_{i-1} \u0026amp; -\\sin\\alpha_{i-1} \u0026amp; -d_i \\sin\\alpha_{i-1} \\ \\sin\\theta_i \\sin\\alpha_{i-1} \u0026amp; \\cos\\theta_i \\sin\\alpha_{i-1} \u0026amp; \\cos\\alpha_{i-1} \u0026amp; d_i \\cos\\alpha_{i-1} \\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\end{align} $$\n最终的末端执行器坐标系 ${N}$ 相对于基坐标系 ${0}$ 的总变换矩阵 $\\mathbf{T}_N^0$ 是所有连杆变换矩阵的连乘：\n$$ {}_N^0\\mathbf{T} = {}_1^0\\mathbf{T} {}_2^1\\mathbf{T} \\cdots {}_N^{N-1}\\mathbf{T} = \\begin{pmatrix} \\mathbf{R} \u0026 \\mathbf{P} \\\\ \\mathbf{0}^T \u0026 1 \\end{pmatrix} $$其中 $\\mathbf{R}$ 是 $3 \\times 3$ 的旋转矩阵（表示姿态），$\\mathbf{P}$ 是 $3 \\times 1$ 的位置向量。\n机械臂逆运动学 (Inverse Kinematics, IK) # 为了实现对机器人的任务规划和控制（“我要到那里，关节应该怎么动？”），逆运动学输入末端执行器的位姿矩阵 $\\mathbf{T}$，输出所有关节变量 $\\theta_i$ 或 $d_i$。\n这个问题的解非唯一确定。如图，我们已知了 $(x,y)$，若要求出两个关节的参数，那么会有至少两组解，即图中的实线和虚线。通常需要根据实际约束（如关节限制、避障、最快/最节能等）来选择最优解。\n求解问题：\nReachable workspace: 可达空间 Dexterous workspace: 手臂可以以任意姿态到达的空间 求解方法 # IK 求解主要分为两类：\n解析法（Analytic Solutions） # 适用于具有特定几何特征（如球形手腕）的机器人，能够得到精确的闭式解 (Closed-form solution)。能由闭式解的话求解速度很快，因此目前大多机械手臂都设计成具有解析解。\n解耦： 通过将机器人分解为手臂（位置）和手腕（姿态）两部分来简化问题。 手臂位置：通常利用几何法或代数法求解前三个关节角。 手腕姿态：对于具有球形手腕（三个关节轴线交于一点）的机器人，可以通过逆变换或欧拉角分解独立求解后三个关节角。 几何法：直接利用几何关系（如余弦定理）求解角度。 代数法：通过将 $\\mathbf{T}_N^0$ 矩阵的元素与 FK 连乘展开后的三角函数表达式进行匹配和求解。 代数法主要通过左乘连杆变换矩阵的逆，逐步分离关节变量：\n$$ {}_1^0\\mathbf{T}^{-1} \\cdot {}_N^0\\mathbf{T} = {}_2^1\\mathbf{T} \\cdots {}_N^{N-1}\\mathbf{T} $$几何法通过余弦定理求解关节角。感觉是个纯平面几何的问题，就不多说了。\n$$ \\cos\\theta_2 = \\frac{x^2 + y^2 - l_1^2 - l_2^2}{2 l_1 l_2} \\quad \\implies \\theta_2 = \\pm \\text{atan2}\\left(\\sqrt{1 - \\cos^2\\theta_2}, \\cos\\theta_2\\right) $$ 数值法（Numerical Solutions） # Note 感觉这一块儿理论和数学的内容很多。实际上要掌握调库应该就可以\n适用于几何结构复杂、无法得到解析解的机器人，通过迭代逼近来寻找近似解。\n基础：基于迭代优化，通常需要机器人雅可比矩阵 $\\mathbf{J}$。 过程：通过计算末端位姿误差 $\\Delta \\mathbf{X}$，并利用 $\\Delta \\mathbf{X} = \\mathbf{J} \\Delta \\mathbf{\\Theta}$ 来迭代更新关节角 $\\mathbf{\\Theta}$，直到误差足够小。 雅可比矩阵 $\\mathbf{J}$：描述关节速度与末端执行器线速度和角速度之间的关系，是 IK 数值解法的核心。 缺点：初始猜测值敏感、收敛速度慢、可能陷入局部最优解、计算量大。 微分运动学关系：末端执行器的广义速度 $\\dot{\\mathbf{X}}$ 与关节速度 $\\dot{\\mathbf{\\Theta}}$ 的关系：\n$$ \\dot{\\mathbf{X}} = \\mathbf{J}(\\mathbf{\\Theta}) \\dot{\\mathbf{\\Theta}} $$IK 数值迭代，利用雅可比矩阵的伪逆 $\\mathbf{J}^{+}$ 来求解关节修正量 $\\Delta\\mathbf{\\Theta}_k$\n$$ \\mathbf{\\Theta}_{k+1} = \\mathbf{\\Theta}_k + \\Delta\\mathbf{\\Theta}_k \\quad \\text{其中} \\quad \\Delta\\mathbf{\\Theta}_k = \\mathbf{J}(\\mathbf{\\Theta}_k)^{+} \\Delta\\mathbf{X}_k $$雅可比伪逆 (Moore-Penrose Pseudoinverse)。当 $\\mathbf{J}$ 是 $m \\times n$ 矩阵时：\n$$ \\mathbf{J}^{+} = \\mathbf{J}^T (\\mathbf{J} \\mathbf{J}^T)^{-1} \\quad (\\text{当 } n \u003c m \\text{ 时，左伪逆}) \\mathbf{J}^{+} = (\\mathbf{J}^T \\mathbf{J})^{-1} \\mathbf{J}^T \\quad (\\text{当 } n \u003e m \\text{ 时，右伪逆}) $$误差由相对变换矩阵 $\\mathbf{T}_{\\text{error}}$ 提取（包含位置误差 $\\Delta\\mathbf{p}$ 和姿态误差 $\\Delta\\boldsymbol{\\phi}$）\n$$ \\mathbf{T}_{\\text{error}} = \\mathbf{T}_{\\text{current}}^{-1} \\mathbf{T}_{\\text{target}} = \\begin{pmatrix} \\mathbf{R}_e \u0026 \\Delta\\mathbf{p} \\\\ \\mathbf{0}^T \u0026 1 \\end{pmatrix} \\quad \\implies \\Delta\\mathbf{X}_k = \\begin{pmatrix} \\Delta\\mathbf{p} \\\\ \\Delta\\boldsymbol{\\phi} \\end{pmatrix} $$ Code # 实际的 KDL、Track-IK 或 IKFast 库通常需要复杂的机器人描述文件 URDF 文件。实际机械臂的 D-H/MDH 参数多达 $4N$ 个（$N$ 是关节数）\n使用 pykdl-all 可以构造一个简单的 KDL 运动链。\nimport PyKDL as kdl import numpy as np # --- 1. Define the Robot Kinematic Chain --- # Assuming a simple 3R arm (three Revolute joints) # Joint axes are all rotating around Z-axis # kdl.Joint() constructor argument: type (RotZ) chain = kdl.Chain() # Link 1: Joint rotates around Z, Link length 1.0 (along X) chain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(1.0, 0.0, 0.0, 0.0))) # kdl.Frame.DH(a, alpha, d, theta) - corresponds to D-H parameters # Link 2: Joint rotates around Z, Link length 1.0 (along X) chain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(1.0, 0.0, 0.0, 0.0))) # Link 3: Joint rotates around Z, Link length 0.5 (as end effector) chain.addSegment(kdl.Segment(kdl.Joint(kdl.Joint.RotZ), kdl.Frame.DH(0.5, 0.0, 0.0, 0.0))) # --- 2. Initialize Solvers --- # Define number of joints (N=3) num_joints = chain.getNrOfJoints() # Joint arrays (kdl.JntArray is KDL\u0026#39;s vector type) q_init = kdl.JntArray(num_joints) q_out = kdl.JntArray(num_joints) # Initial Joint Guess q_init[0], q_init[1], q_init[2] = 0.1, 0.1, 0.1 # Forward Kinematics Solver (required) fk_solver = kdl.ChainFkSolverPos_recursive(chain) # Inverse Kinematics Velocity Solver (required for numerical IK) ik_v_solver = kdl.ChainIkSolverVel_pinv(chain) # Inverse Kinematics Position Solver (Numerical Iterative Solver: Newton-Raphson) # Max Iterations = 100, Epsilon = 1e-6 (Convergence precision) ik_p_solver = kdl.ChainIkSolverPos_NR(chain, fk_solver, ik_v_solver, 100, 1e-6) # --- 3. Define Target Frame --- # Target Position P(x, y, z) = (1.5, 0.5, 0.0) P_target = kdl.Vector(1.5, 0.5, 0.0) # Target Orientation R (No rotation / Identity) R_target = kdl.Rotation.RPY(0.0, 0.0, 0.0) # Target Homogeneous Transformation Matrix T_target T_target = kdl.Frame(R_target, P_target) # --- 4. Execute IK Solution --- # Call IK solver: (initial angles, target pose, output angles) status = ik_p_solver.CartToJnt(q_init, T_target, q_out) # --- 5. Output Results --- print(\u0026#34;--- PyKDL (Numerical IK) Demonstration ---\u0026#34;) print(f\u0026#34;Target Position: ({P_target[0]:f}, {P_target[1]:f}, {P_target[2]:f})\u0026#34;) if status \u0026gt;= 0: print(\u0026#34;Solution SUCCESSFUL!\u0026#34;) thetas = [q_out[i] for i in range(num_joints)] print(f\u0026#34;Solved Joint Angles (rad): {np.round(thetas, 4)}\u0026#34;) # Verification (FK) T_check = kdl.Frame() fk_solver.JntToCart(q_out, T_check) print(f\u0026#34;Verification Position (FK): ({T_check.p[0]:f}, {T_check.p[1]:f}, {T_check.p[2]:f})\u0026#34;) else: print(\u0026#34;Solution FAILED! Try changing the initial guess or target pose.\u0026#34;) ","date":"16 October 2025","externalUrl":null,"permalink":"/posts/ai/eai/robotics/%E6%9C%BA%E6%A2%B0%E8%87%82%E8%BF%90%E5%8A%A8%E5%AD%A6/","section":"Posts","summary":"","title":"机械臂运动学","type":"posts"},{"content":"","date":"11 October 2025","externalUrl":null,"permalink":"/tags/robotics/","section":"Tags","summary":"","title":"Robotics","type":"tags"},{"content":"","date":"11 October 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" 欧拉角的万向锁现象 # 看了很多篇博客，很多都举了转手机的例子，但是本人天生愚笨，一点也没看懂。今天细细研究才发现原来漏了关键的一点啊，而很多博客竟都对此讳莫如深。\n转手机实验 用自己的手机做一个试验，你把手机屏幕朝上，手机的长边为 Z 轴，短边为 Y 轴，X 轴垂直屏幕向下，那先绕 X 轴（垂直屏幕）旋转一下手机，假设旋转 30 度，然后再把手机绕 Y 轴（短边）旋转 90 度，也就是把手机短边接触桌面竖立起来，这时候你再绕 Z 轴（长边）以任意角度旋转，你会发现手机的 Y 轴（短边）一直定在桌面上不可能脱离桌面，这就是万向锁现象。\n很蒙对不对！说实话我也没搞懂，但是网上的资料都是这么写的。😭不怪你，是它不够严谨，后面我会解释的。\n分析一下这个过程：\n你先绕 X 轴旋转 30∘（这个角度不重要）。 关键是绕 Y 轴旋转 90∘，这使得原来的 Z 轴（垂直屏幕向下）现在和原来的 Z 轴（长边）对齐了，并且它们都与桌面平行。 现在的结果： 当你再尝试绕 Z 轴（长边）以任意角度旋转时，你发现手机的 Y 轴（短边）始终贴在桌面上。这表明绕 Z 轴的旋转本质上与绕 X 轴的旋转是同一个动作，只是方向相反或相同。那你要问了：第三点里怎么能是一个动作呢？现在绕 X 轴（也就是垂直屏幕的方向）旋转明明可以把手机的短边抬离桌面啊！ 诶！这就是核心点了。欧拉角这个逆天的设定有最重要的一条，而这一条全网大多数人没说，只有下面的这个视频说了：前面（或者说外部轴）的转动，会带着后面（内部轴）一起转，而内部轴不能带动外部轴转动！比如在 ZYX 欧拉角中，我们转 $x$ 轴，那么 $y$、$z$ 轴要跟着转；而转 $y$ 轴只有 $z$ 轴跟着转，转 $z$ 轴谁都不跟着转。\n为什么要这样逆天的设定呢？这是人为设定的吗？还真不是。我们说欧拉角变换是一个变换“过程”，变换都是相对于初始状态，变换的顺序很重要——比如我们规定先 Z 再 Y 后 X 旋转，那么 $(30,90,0)$ 这个变换和 $(1,30,90)$ 这个变换，都得先对 $z$ 轴旋转，再对 $y$、$x$ 旋转，不能说第一个变换看见 $z$ 轴没转或转的少，那就直接跳过它先转 $y$、$x$ 二轴。\n这就解释清楚了为什么外部轴带动内部轴转动。要是还不清楚，看下面的视频吧。\n我们再用数学解释一下。这个数学也许跟网上的数学不太一样，如果想看旋转 90 度角是怎么影响万向锁转向的，请任搜一篇博客。我们讲的东西在《机器人学导论（原书第三版）》的 P43，参照本节开头的例子，使用 Z-Y-X 欧拉角，即规定了绕轴的顺序。\n我们很容易知道欧拉角的变换就相当于是左乘一个旋转矩阵，那么整个旋转过程就相当于：\n$$ {}^A_{B}R={}^A_{B'}R \\space {}^{B'}_{B''}R \\space {}_{B}^{B''}R $$其中 $B\u0026rsquo;\u0026rsquo;$ 和 $B\u0026rsquo;$ 都是转过对应轴的中间过程。\n本文里不妨直接写作 ${}^A_{B}R = R_{x}(\\phi) R_{y}(\\theta) R_{z}(\\psi)$ 吧，参考开头的飞机图。注意，旋转变换是不断左乘一个矩阵，所以先进行的变换靠右，而后进行的变换乘在左边。\n🤓👆写到这里你应该悟了吧！Z-Y-X 欧拉角中，我们对 $z$ 轴旋转完成，然后对 $y$ 轴旋转的时候，由于矩阵乘法 $R_{y}(\\theta) R_{z}(\\psi)$，势必会同时影响到 $z$ 轴，让它跟着一起转，而并不会影响到后转的 $x$ 轴（此时 $R_{x}(\\phi)$ 还没有乘进来）。\n重新回顾一下转手机的过程：Z-Y-X 欧拉角，先绕 x 轴旋转 30 度导致 y 轴 z 轴都跟着转（注意此时 x 轴垂直桌面），再绕 y 轴旋转 90 度，此时 z 轴会跟着旋转到垂直桌面，而 x 轴没变，依然垂直桌面！最后我们转 x 轴和 z 轴得到的结果就是一样的了。🙋看到了吗！我们丢失了一个自由度！！这就是我们朝思暮想的万向锁啊！！\n万向死锁的表现是：丢失了一个轴 (或叫丢失了一个自由度)；是欧拉角表达不唯一的重要体现形式之一。\n本文采用 CC BY 4.0 国际许可协议 进行许可。转载请注明原文作者、出处及本文链接。\n","date":"11 October 2025","externalUrl":null,"permalink":"/posts/ai/eai/robotics/%E6%AC%A7%E6%8B%89%E8%A7%92%E7%9A%84%E4%B8%87%E5%90%91%E9%94%81%E7%8E%B0%E8%B1%A1/","section":"Posts","summary":"","title":"欧拉角的万向锁现象","type":"posts"},{"content":" 如何嵌入 B 站视频 # 本来不必写这篇博客的，因为网上有很多方法。直到 2025 年 10 月 10 日，我发现 b 站删除了网页端的分享按钮！那么网上的很多方法都失效了，我之前看过的、教人如何在 obsidian 插入 b 站视频的博客也随之删除了。\n那我只有自己写一下了，以免 b 站以后删除更多 API。\n参考：\nBilibili 弹幕文件的解析 关于博客园内嵌入bilibili视频 - 王陸 - 博客园 这是官方准备的嵌入代码，目前看来还没封，可以直接拿来用：\n\u0026lt;iframe src=\u0026#34;https://player.bilibili.com/player.html?isOutside=true\u0026amp;bvid=BV1Nr4y1j7kn\u0026amp;p=1\u0026amp;autoplay=false\u0026#34; scrolling=\u0026#34;no\u0026#34; border=\u0026#34;0\u0026#34; frameborder=\u0026#34;no\u0026#34; framespacing=\u0026#34;0\u0026#34; allowfullscreen=\u0026#34;true\u0026#34; width=\u0026#34;95%\u0026#34; height=600\u0026gt;\u0026lt;/iframe\u0026gt; 使用方法：把上面的链接中，aid、cid 或者 bvid 替换成你视频的对应项，三选一即可。其中 bvid 可以通过页面 url 直接拿到（那个大小写数字混合的就是 bvid，如 BV1Wv3xeNEds）。\n显然写 bvid 是最方便的。其余的拿不到，但为了防止 b 站更改 api，本博客也记一下其他的可以通过以下这个链接查到：\nhttps://api.bilibili.com/x/web-interface/view?bvid= 常用的参数（加在 src 里面，如上面例子的 autoplay=false）\nkey 说明 page 第几个视频，起始下标为 1 （默认值也是为 1)）就是 B 站视频，选集里的，第几个视频 as_wide 是否宽屏。1： 宽屏，0:：小屏 high_quality 是否高清。1：高清，0：最低视频质量 (默认) 如视频有 360p 720p 1080p 三种，默认或者 high_quality=0 是最低 360p high_quality=1 是最高 1080p danmaku 是否开启弹幕。1：开启 (默认)，0：关闭 autoplay 是否自动播放 ","date":"10 October 2025","externalUrl":null,"permalink":"/posts/dev/%E5%A6%82%E4%BD%95%E5%B5%8C%E5%85%A5b%E7%AB%99%E8%A7%86%E9%A2%91/","section":"Posts","summary":"","title":"如何嵌入b站视频","type":"posts"},{"content":" 旋转矩阵、欧拉角、四元数 # 哎！感慨一下当时学图形学和 Unity 的时候就要会这些概念，可惜当时没好好学。现在老大徒伤悲了。\n旋转矩阵 # 坐标系与位姿变换 里说过这一节了。本质上就是 ${}_{B}^AR$，表示从 $B$ 坐标系到 $A$ 坐标系的旋转变换。\n一个矩阵 R 是旋转矩阵，当且仅当它满足以下两个条件：\n正交矩阵（Orthogonal Matrix）： 它的逆矩阵等于它的转置矩阵，即 $R^{-1}=R^T$。 这保证了旋转是等距变换（保持向量长度不变， $∥Rv∥=∥v∥$）。 行列式为 1： $det(R)=1$。 这保证了旋转是真正的旋转，而不是旋转加反射（如果行列式为 -1，则包含了一个反射）。 在二维平面上，将一个向量 $v=(x,y)$ 逆时针旋转 $θ$ 角，旋转后的新向量 $v\u0026rsquo;=Rv$。对应的旋转矩阵 $R$ 为： $$ \\mathbf{R}(\\theta) = \\begin{pmatrix} \\cos\\theta \u0026 -\\sin\\theta \\\\ \\sin\\theta \u0026 \\cos\\theta \\end{pmatrix} $$ 绕 X 轴旋转 (Roll)\n$$ \\mathbf{R}_x(\\theta) = \\begin{pmatrix} 1 \u0026 0 \u0026 0 \\\\ 0 \u0026 \\cos\\theta \u0026 -\\sin\\theta \\\\ 0 \u0026 \\sin\\theta \u0026 \\cos\\theta \\end{pmatrix} $$绕 Y 轴旋转 (Pitch)\n$$ \\mathbf{R}_y(\\theta) = \\begin{pmatrix} \\cos\\theta \u0026 0 \u0026 \\sin\\theta \\\\ 0 \u0026 1 \u0026 0 \\\\ -\\sin\\theta \u0026 0 \u0026 \\cos\\theta \\end{pmatrix} $$绕 Z 轴旋转 (Yaw)\n$$ \\mathbf{R}_z(\\theta) = \\begin{pmatrix} \\cos\\theta \u0026 -\\sin\\theta \u0026 0 \\\\ \\sin\\theta \u0026 \\cos\\theta \u0026 0 \\\\ 0 \u0026 0 \u0026 1 \\end{pmatrix} $$$v$ 先绕 $X$ 再绕 $Y$：$v\u0026rsquo;=\\mathbb{R}{x}R{y}v$\n旋转矩阵缺点比较多：\n不直观 矩阵占据了更多的内存 由于浮点精度的限制，大量的矩阵乘法最终可能导致病态矩阵，这种现象称作“矩阵蠕变”，矩阵正交化能解决矩阵蠕变的问题，因为乘起来都是 0 了。 欧拉角 # 欧拉角是带有顺序的三元组。欧拉角的定义很不同，有静态和动态定义。动态定义指的是绕物体本地的坐标轴进行旋转（如动图 2）。航空领域通常采用的旋转方式是 ZYX 顺序（称作 Tait-Bryan angles），下图是一架飞机按照 ZYX 组合进行旋转产生欧拉角的过程，其中，ψ为偏航角（yaw），θ为俯仰角（pitch），φ为滚转角（roll）。\n欧拉角的优势是直观，要显示和键盘输入方位时，欧拉角是唯一的选择，但劣势是：\n给定方位的表达方式不唯一（欧拉角的万向锁现象） 角度之间求插值非常困难 四元数 Quaternions # Understanding Quaternions | 3D Game Engine Programming\n二维平面上的旋转：复数 # 复数基础与二维空间旋转 - 何雨龙 - 博客园\n四元数的思想来源于复数。高中学过，二维平面上的点 $(a,b)$ 可以用复数表示为 $z = a + bi$，而写成极坐标就很容易看出和旋转的关系：\n$$ z = a+bi = r\\cos \\theta + ir \\sin \\theta = r(\\cos \\theta+i\\sin\\theta) $$由欧拉公式\n$$ e^{i\\theta}=\\cos \\theta + i \\sin \\theta $$上式可写作\n$$ z = re^{i\\theta} $$$r$ 指的是向量的模长，$\\theta$ 是幅角，即与 $x$ 轴之间的夹角。\n两个复数相乘表示旋转。\n$$ z_{1}z_{2}=r_{1}r_{2}e^{i({\\theta_{1}+\\theta_{2})}} $$即相乘之后模长变为原模长的乘积，幅角变为原幅角之和。\n那么表示纯旋转的旋转角显然是模长为 1 只有纯角度的单位复数。被称为旋转子（Rotator）：\n$$ R_{\\theta}=\\cos \\theta + i \\sin \\theta = e^{i\\theta} $$如点 $(1,3)$ 进行 $\\frac{\\pi}{4}$ 的旋转：$(1+3i) * e^{i \\frac{\\pi}{4}}=(1+3i)*\\left( \\frac{\\sqrt{ 2 }}{2}+i \\frac{\\sqrt{ 2 }}{2} \\right)$\n三维空间旋转：四元数 # 运算基础 # 四元数形式：\n$$ \\begin{align} q \u0026= s + x i + y j + z k \\\\ q \u0026= [s, \\mathbf{v}], s \\in R, v \\in R^3 \\end{align} $$类似地，\n$$ \\begin{align} i^{2}=j^{2}=k^{2}=ijk=-1 \\ ij=k,ji=-k \\ jk=i,kj=-i \\ ki=j,ik=-j \\\n\\end{align} $$\n这种关系很像是三维坐标系下，三个单位向量叉乘的关系：$\\mathbf{x} \\times \\mathbf{y}=\\mathbf{z}, \\space \\mathbf{y} \\times \\mathbf{x} = \\mathbf{-z}$ 这种，两两叉乘为剩余的正交向量。\n四则运算遵循复数的法则，在这里说一下乘法。稍加推导可得：\n$$ [s_{a},\\mathbf{a}][s_{b}, \\mathbf{b}] = [s_{a}s_{b} - \\mathbf{a}\\mathbf{b}, s_{a}\\mathbf{b}+s_{b}\\mathbf{a}+a \\times b] $$用 $q^*$ 表示共轭四元数：\n$$ \\begin{align} q\u0026=[s,\\mathbf{v}] \\\\ q^*\u0026=[s,\\mathbf{-v}] \\\\ qq^*\u0026=[s^{2}+v^{2},\\mathbf{0}]=|q|^{2} \\end{align} $$四元数的逆\n$$ \\begin{align} q^{-1}\u0026=\\frac{q^*}{|q|^{2}} \\\\ qq^{-1}\u0026=\\frac{qq^*}{|q|^{2}}=1 \\end{align} $$ 旋转 # 空间中，三维向量表示为纯四元数，即实部为 0 的四元数：\n$$ \\mathbf{v}_{q}=[0, \\mathbf{v}] $$一个表示旋转的单位四元数 $\\mathbf{q}$ 可以写成：\n$$ \\mathbf{q} = \\cos\\left(\\frac{\\theta}{2}\\right) + \\mathbf{u} \\sin\\left(\\frac{\\theta}{2}\\right) $$在进行旋转操作时，就是将这个实部为零的四元数 $\\mathbf{p}_q$ 夹在旋转四元数 $\\mathbf{q}$ 及其逆（或共轭，因为旋转四元数为单位四元数，所以 $|q|=1$） $\\mathbf{q}^{-1}$ 之间进行计算：\n$$ \\mathbf{v}'_q = \\mathbf{q} \\mathbf{v}_q \\mathbf{q}^{-1} $$得到的 $\\mathbf{v}\u0026rsquo;_q$ 依然为纯四元数，其中虚部就是旋转之后向量的坐标。实部为 1 的四元数表示 0 度旋转。\n相互转换 # 知乎-鸡哥-三维旋转：欧拉角、四元数、旋转矩阵、轴角之间的转换\n","date":"10 October 2025","externalUrl":null,"permalink":"/posts/ai/eai/robotics/%E5%A7%BF%E6%80%81%E6%8F%8F%E8%BF%B0%E6%97%8B%E8%BD%AC%E7%9F%A9%E9%98%B5%E6%AC%A7%E6%8B%89%E8%A7%92%E4%B8%8E%E4%B8%87%E5%90%91%E9%94%81%E5%9B%9B%E5%85%83%E6%95%B0/","section":"Posts","summary":"","title":"姿态描述：旋转矩阵、欧拉角与万向锁、四元数","type":"posts"},{"content":" 坐标系与位姿变换 # 这一章相关内容在《机器人学导论（原书第三版）》2.2 节。\nPreliminary Notations # $^{A}P$ ，$P$ 表示 Position，一个位置坐标。左上角的符号表示在 $A$ 坐标系下。 ${}_{B}^AR$，$R$ 表示 Rotation，旋转，和平移一起统称叫变换（后面有个字母 $T$ 就涵盖了这两个过程）。左下角和左上角的坐标，表示 $B$ 坐标系相对于 $A$ 坐标系的变换。变换显然是个动词，所以肯定得有相对的概念。 总之，这一张明白什么是坐标系 Frame，什么是位置 Position，什么是变换 Transform 就行。 位置\u0026amp;姿态：描述机器人状态的基石 # 我们把机械臂想象成人体的大臂 + 小臂 + 手，那么很容易观察到这么一个现象：大臂绕躯干运动，小臂绕大臂运动，手绕小臂运动。一个机械臂就是这么组成的。我们如果简化一下，假想人的小臂大臂都为刚体（高中物理竞赛学的概念，如果不那么好想那就想象一条钢管），手和小臂是一个整体（统称为手臂），灵活地绕着肘关节活动，那么便抽象出了机器人学里位置与姿态的概念。\n位置就是大臂之于躯干的位置，如果我们把躯干叫做世界 s 坐标系 World Frame的话，就很容易知道位置可以用一个空间中的三维向量 $P=\\begin{bmatrix}P_{x} \\ P_{y} \\ P_{z}\\end{bmatrix}$ 来表示。这一块儿太简单了，就像我们很容易知道肘关节在躯干的哪里一样。我们把世界坐标系用 $A=\\begin{bmatrix}\\hat{X_{A}} \\ \\hat{Y_{A}} \\ \\hat{Z_{A}}\\end{bmatrix}$ 来表示。\n怎么说呢…我觉得完全可以把 $A$ 当作世界坐标系，但是书上似乎不是这么写的，他追求了更普遍的表示——用 $U$（Universe） 表示世界坐标系。后面注意一下就行。\n而姿态，就是小臂相对于肘关节的位置。很容易想到手（机械臂末端）在小臂的作用下可以平移，在小臂和手腕的共同作用下可以旋转，很灵巧。相对于机械臂末端执行器（也就是肘关节）来说，我们可以给末端（手）建立一个局部坐标系 Body Frame，来描述它的姿态——是如何相对于末端执行器进行各种妖娆的平移和旋转的。把局部坐标系用 ${}^AB=\\begin{bmatrix}{}^A\\hat{X_{B}} \\ {}^A\\hat{Y_{B}} \\ {}^A\\hat{Z_{B}}\\end{bmatrix}$ 表示。\n这里有几点。第一，我自认为局部坐标系没有讲清楚，那么请把大拇指、食指和中指三个指头充分张开，就像判定磁感线方向那样。这就是一个局部坐标系 B，接下来感受一下它是如何相对于你身体的世界坐标 t 系运动的；第二，局部坐标系的左上标 ${}^AB$，表示B 相对于 A。\n坐标变换的一般式 # 旋转变换是左乘一个旋转矩阵。左上角的 $A$ 表示在 $A$ 坐标系中，那 ${}^A \\mathbf{\\hat{X}}_B$ 就指的是在 $A$ 坐标系中表示 $B$ 坐标系单位矢量的坐标。\n$$ {}^A_{B} \\mathbf{R} = \\begin{bmatrix} \\vert \u0026 \\vert \u0026 \\vert \\\\ {}^A \\mathbf{\\hat{X}}_B \u0026 {}^A \\mathbf{\\hat{Y}}_B \u0026 {}^A \\mathbf{\\hat{Z}}_B \\\\ \\vert \u0026 \\vert \u0026 \\vert \\end{bmatrix} $$ 旋转变换 矢量 $P$ 从 $B$ 坐标系表示旋转变换到 $A$ 坐标系表示。下面的 $R$ 指的是 Rotate。 $$ \u003e {}^A P = {}_{B}^{A}R {}^{B}P \\tag{1} \u003e $$ 正交矩阵的逆等于转置 所以 ${}^A_{B}R={}{A}^BR^{-1}={}{A}^BR^{T}$。也就是说坐标系 $B$ 相对于坐标系 $A$ 的变换矩阵，等于 $A$ 坐标系相对于 $B$ 坐标系的变换矩阵的逆。\n平移变换过于简单，直接加一个平移矢量就行。\n综合变换（位姿变换） 矢量 $P$ 从 $B$ 坐标系表示综合平移和旋转变换到 $A$ 坐标系表示。 $$ \u003e {}^A P = {}^A_{B} R {}^B P + {}^A P_{B\\mathbf{ORG}} \\tag{2} \u003e $$ 这个 ORG 的意思是 origin，原点，表示 A 坐标系表示下 B 的原点的坐标。\n旋转控制姿态，平移控制位置。\n坐标变换齐次式 # 这块在相机的内外参，以及机器学习里都有这样的技巧，那就是我们上节的非齐次式不好看，那就扩张一个常数维度，变成齐次的：\n$$ {}^A P = {}_{B}^{A} T \\space \\mathbf{}{}^BP $$这个 $T$ 是 4*4 的矩阵，称为齐次变换矩阵。上式亦即：\n$$ \\begin{bmatrix} {}^A \\mathbf{P} \\ 1 \\end{bmatrix} # \\begin{bmatrix} \\begin{array}{c|c} {}^A_B \\mathbf{R} \u0026amp; {}^A \\mathbf{P}_{B\\mathbf{ORG}} \\ \\hline 0\\ 0\\ 0 \u0026amp; 1 \\end{array} \\end{bmatrix} \\begin{bmatrix} {}^B \\mathbf{P} \\ 1 \\end{bmatrix} \\tag{3} $$\n这种方法让计算机忙于冗余的 01 乘法，不如 (2) 快，所以仅用作简化理论推导。\n","date":"9 October 2025","externalUrl":null,"permalink":"/posts/ai/eai/robotics/%E5%9D%90%E6%A0%87%E7%B3%BB%E4%B8%8E%E4%BD%8D%E5%A7%BF%E5%8F%98%E6%8D%A2/","section":"Posts","summary":"","title":"坐标系与位姿变换","type":"posts"},{"content":" 估计量的评优 # 均方误差 MSE、偏差 Bias 和方差 Var # 怎么说呢…学到这里我才发现 PRML 对数理统计的要求太高了。这些东西都应该作为前置知识提到的，在 PRML 里都是直接应用了。\n数理统计的教材上提到了 MSE 的诞生动机。对于一个估计量 $\\hat{\\theta}$，我们显然想让它尽可能地逼近真实的随机变量 $\\theta$。如何衡量逼近的程度？显然 $|\\hat{\\theta}-\\theta|$ 也就是二者差的绝对值是比较好想的。但是绝对值在数学上不好计算，就改成 $(\\hat{\\theta}-\\theta)^{2}$ 吧。但 $\\hat{\\theta}$ 是随机变量（课本上说 $\\theta$ 是随机的，我觉得不对，$\\theta$ 应该是这个分布的 ground truth，是一个常数值，在 MSE 偏差 - 方差分解推导会用到。而 $\\hat{\\theta}$ 由于样本的区别是随机变量），如何消除随机性呢？那就在所有 $\\theta$ 上取期望：\n$$ MSE(\\hat{\\theta})=E[|\\hat{\\theta}−\\theta|^2] $$这就是 MSE 的由来。\n推导偏差 - 方差分解式就是把它展开，只需要注意交叉项会被抵消为 0 即可。\n估计量评估的基石：偏差 - 方差分解（Bias-Variance Decomposition） # $$ MSE(\\hat{\\theta})=Var(\\hat{\\theta})+[Bias(\\hat{\\theta}, \\theta)]^{2} $$均方误差=方差 Var + 偏置 Bias 的平方。 这是最最最重要的一个公式。\n方差：\n$$ Var(\\hat{\\theta})=E[|\\hat{\\theta}-E(\\hat{\\theta})|^{2}] $$注意和 MSE 的区别，方差指的是估计量 $\\hat{\\theta}$ 的方差，衡量的是你这个估计是不是稳定，不能 $\\hat{\\theta}$ 今天估计成 2，明天直接变成 2k ，后天又变成 0.2 了。\n偏差平方：\n$$ [Bias(\\hat{\\theta}, \\theta)]^{2}=(E[\\hat{\\theta}]-\\theta)^{2} $$ 注意这个 Bias 有两个输入，一个是估计值（统计量）$\\hat{\\theta}$，一个是 ground truth $\\theta$\n就很显然了，表达的是你这个估计 $\\hat{\\theta}$ 的期望，距离真实 Ground truth 的 $\\theta$ 到底差了多少。比如估计的 $\\hat{\\theta}=2$，结果真实 $\\theta=2k$，那这就差之远矣。如果 $Bias\\equiv 0$，那么意味着估计量的均值准确（注意不是估计量准确，而是其均值准确），被称为无偏估计。若参数 $\\theta$ 的无偏估计存在，则称 $\\theta$ 为可估的。\n书上是这么写的：\n$$ MSE_{\\theta}(T(x))=Var_{\\theta}(T(x))+b^{2}(q(\\theta),T) $$其中 $q(\\theta)$ 是 Ground Truth（大概是想说 quantity，以 $\\theta$ 为参数），$T$ 是统计量。意思是一个意思。\n估计量的评估和非容许 # 一个估计量 $S(x)$ 被称为非容许的（Non-admissible），是因为存在另一个估计量 $T(x)$，在均方误差的意义上，全面不差于 $S(x)$，且至少有一点要优于 $S(x)$。\n需要满足两个条件：\n全面不差。MSE 显然是越小越好： $$ MSE_{\\theta} ​(T(x)) \\leq MSE_{\\theta}​ (S(x)), \\space \\forall \\theta \\in \\Theta $$ 严格更优。至少有一点要更好： $$ MSE_{\\theta} ​(T(x)) \\lt MSE_{\\theta}​ (S(x)), \\space \\exists \\theta \\in \\Theta $$ 一致最小方差无偏估计（UMVUE） # UMVUE 是由三个定语共同限定的：\n定语 英文简称 含义 统计学意义 无偏 (Unbiased, U) $E[\\hat{\\theta}] = \\theta$ 估计量的期望值等于被估计参数 $θ$ 的真值。 保证估计量不具有系统性偏差，是“准确”的。 最小方差 (Minimum Variance, MV) $Var(\\hat{\\theta}) \\leq Var(\\tilde{\\theta})$ 在所有无偏估计量中，它的方差最小。 保证估计量在多次重复抽样时具有最高的“精确度”或“稳定性”。 一致 (Uniformly, U) 对所有可能的参数值 $\\theta$ 均成立。 最小方差的性质对参数空间中的所有 $θ$ 都成立。 保证估计量的最优性不依赖于 $θ$ 的真实值。 判断统计量 $\\hat{\\theta}$ 是否是 UMVUE 的充要条件（存在性） 一个无偏估计量 $T(X)$ 要想成为 UMVUE，它必须与所有期望为零、方差有限的统计量 $T_{0}(X)$ 不相关，即 $Cov(T(X),T_{0}(X))=0$\n唯一性 在概率 1 下，UMVUE 是唯一的。即如果 $T(X)$ 和 $S(X)$ 都是充分统计里，那么 $P_{\\theta}{T(X)=S(X)}=1$\n在概率 1 下 的理解 概率为 0 的事也可以发生。因为当我们说一个事件的概率为 0，实际上指的是这个事件在事件域上的测度为 0。比如在连续样本空间中，任何一个单点的测度都为 0。反过来，概率为 1 的事也有可能不发生。\n","date":"30 September 2025","externalUrl":null,"permalink":"/posts/ai/statics/%E4%BC%B0%E8%AE%A1%E9%87%8F%E7%9A%84%E8%AF%84%E4%BC%98/","section":"Posts","summary":"","title":"估计量的评优","type":"posts"},{"content":" 充分统计量 # 充分统计量-知乎\n什么是充分统计量 # 统计量：$(X_{1},X_{2},\\dots,X_{n})$ 是样本，如果如果由此样本构造一个函数 $T(X_{1},X_{2},\\dots,X_{n})$ 中不含任何未知数，那么它就是样本的统计量。常见的统计量如样本的方差、均值等。\n充分统计量指能浓缩样本信息的量。这是什么意思呢？比如你是一个检察官，你有一箱罪证。你不是把这一箱罪证都报告给上级，而是写成一个凝练的报告包含了罪证的全部信息，如“犯罪人员的平均身高”、“案发地点的平均光照强度”等等。这些量就是“充分统计量”。\n数学上来讲，如果我们已知充分统计量的情况下，样本分布便不含参数，那就叫做浓缩了样本信息。比如我们考虑 $X_{1},X_{2},\\dots,X_{n}\\sim B(1，p)$，如果我们已知了样本之和 $T = \\sum_{i=1}^n{X_{i}} = t$，那么联合分布列：\n$$ P(X_{1},X_{2},\\dots,X_{n}|T=t)= \\frac{P\\left( X_{1}=x_{1},X_{2}=x_{2},\\dots,X_{n}=t-\\sum_{i=1}^{n-1}x_{i} \\right)}{P(T=t)}=\\frac{p^t(1-p)^{n-t}}{C_{n}^{t}p^t(1-p)^{n-t}}=\\frac{1}{C_{n}^t} $$ 为什么是这个式子 分母很简单，知道总共有 t 个 1（二项分布之和为 t），那么有 $C_{n}^t$ 种选法，乘上后面的因子；分子固定了 $x_{1},x_{2},\\dots,x_{n}$，最后一个变量 $X_{n}$ 只能等于 $t-\\sum_{i=1}^{n-1}x_{i}$。注意没有排列组合因子，因为是固定的。\n还是很懵，对吧？举个例子，$n=5$ 的情况下，如果已知五个样本的和为 3，那么 $X_{1},X_{2},\\dots,X_{n}$ 的联合分布将不再是一个函数，而是一个定值——$\\frac{1}{C_{5}^3}=\\frac{1}{10}$。呃，这个值似乎挺好理解，因为知道总共有 5 个数，从中取 3 个为 1 就行，那么它的联合分布就只有 $C_{5}^3$ 种可能性，概率也就是 $\\frac{1}{C_{5}^3}$ 了。至于“联合分布为常数”，意思是：在满足条件（即 $T=t$）的那些样本向量之间，概率是均匀分布的。\n一般来说，样本的均值 $\\bar{X}$、方差这种量都是充分统计量，而 $X_{1}+X_{2}$ 这种就不是，因为它“不能表征样本的全貌”。换句话说即使我们知道了 $X_{1}+X_{2}=t$，那剩下的数字也得依靠参数 $p$ 来求。\n需要知道的是，充分统计量一一变换之后得到的统计量，也是充分统计量。\nQuote 若 $T(x)$ 是充分统计量，$g(t)$ 是一一对应的实函数（可以是向量函数），则 $g(T(x))$ 也是充分统计量。\nExample 例如，如果 $T(x)=\\sum x_{i}^2$ 是充分统计量，那么 $T\u0026rsquo;(x)=\\sum(x_{i}-\\bar{x})^2$ 也是充分统计量。\n求或者判定充分统计量：因式分解定理 # 因式分解定理（Fisher-Neyman 准则） 若联合分布函数 $p(x;\\theta)$，其中 $\\theta$ 为未知参数。则 $T(x)$ 为充分统计量当且仅当存在： $$ \u003e p(x;\\theta)=g(T(x),\\theta)h(x) \u003e $$ Note 简单来讲，我们只要写出联合分布函数 $p(x;\\theta)$，看能不能够分解成两个因式，其中一个因式 $h(x)$ 只与样本 $x$ 有关 （这个 $h(x)$ 可以恒为常数，比如 $h(x)\\equiv 1$），另一个 $g(T(x),\\theta)$ 与样本 $x$ 和参数 $\\theta$ 都有关。\n以上的定理简化了不少内容，不过大致理解足够了。这个分解告诉我们：\n左边：似然函数 $L(θ;x)$（总信息） 这是我们从样本中提取的关于 $θ$ 的所有信息。 右边第一项：$g(T(x);θ)$（充分信息） 这是带有参数 $θ$ 的部分。它必须只通过统计量 $T(X)$ 来影响似然函数的值。这意味着所有与 $θ$ 有关的样本信息都已经被 $T(X)$ 打包进去了。 右边第二项：$h(x)$（无关信息） 。由于它不含 $θ$，所以对我们推断 $θ$ 没有任何帮助。这部分信息是样本数据中固有的，但与我们想估计的参数是无关的。 ","date":"28 September 2025","externalUrl":null,"permalink":"/posts/ai/statics/%E5%85%85%E5%88%86%E7%BB%9F%E8%AE%A1%E9%87%8F/","section":"Posts","summary":"","title":"充分统计量","type":"posts"},{"content":"","date":"26 September 2025","externalUrl":null,"permalink":"/tags/rl/","section":"Tags","summary":"","title":"RL","type":"tags"},{"content":" 霍夫丁不等式 (Hoeffding\u0026rsquo;s inequality)，与多臂赌博机 (MAB) 中的上置信界 (UCB) 算法 # 多臂老虎机\n霍夫丁不等式 (Hoeffding\u0026rsquo;s inequality) # 设有两两独立的随机变量 $X_1, \\dots, X_n !$，那么它们的经验期望值（含义见后）：\n$$ \\overline{X} = \\frac{X_1 + \\cdots + X_n}{n} $$满足：\n$$ \\mathbb{P}(\\overline{X} - \\mathbb{E}[\\overline{X}] \\geq t) \\leq \\exp \\left( - \\frac{2t^2n^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right),\\! $$令 $\\bar{X}=-\\bar{X}$：\n$$ \\mathbb{P}(\\mathbb{E}[\\overline{X}]-\\overline{X} \\geq t) \\leq \\exp \\left( - \\frac{2t^2n^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right),\\! $$进而可得（注意有个绝对值）：\n$$ \\mathbb{P}(|\\overline{X} - \\mathbb{E}[\\overline{X}]| \\geq t) \\leq 2\\exp \\left( - \\frac{2t^2n^2}{\\sum_{i=1}^n (b_i - a_i)^2} \\right),\\! $$其中 $\\mathbb{P}(X_i \\in [a_i, b_i]) \\approx 1 !$，即变量 $X_i$ 几乎必然满足 $a_i\\leqslant X_i \\leqslant b_i ,$\n经验期望 (Empirical Expectation) # 经验期望是根据有限的观测数据来估计总体期望。假设我们有一组数据 $x_1,x_2,…,x_n$，这些数据是来自一个随机变量 $X$ 的独立同分布 (i.i.d.) 样本。那么，经验期望（或者说样本均值）的定义为：\n$$ \\mathbb{\\hat{E}}[X]=\\frac{1}{n}\\sum_{i=1}^{n}x_i $$这个公式表示将所有样本值加起来后除以样本个数，得到的结果就是经验期望，它是对总体期望 $\\mathbb{E}(X)$ 的一个估计。﻿\n多臂赌博机中的上置信界 # 这是在读 《动手学深度学习》 的上置信界算法（upper confidence bound，UCB）时产生的思考。UCB 是怎么取的呢？假设当某一个臂被摇了 n 次（在动作 Action $a$）之后，产生的累计效益 $\\hat{Q}(a)$ 依然很低时，我们值不值得再去摇它呢？\n霍夫丁不等式可以说明这个问题。经验期望是在有限次观察下，该臂所产生的效益 $\\hat{Q}(a)$。注意，我们的 $\\hat{Q}(a) =\\frac{1}{n}\\sum_{i=1}^{n}r_i$，其中 r 指 reward。在观察次数足够多（long term）的时候，$\\hat{Q}(a)$ 应该会趋近于其期望 $\\mathbb{E}[Q(a)]$。注意，经验期望是不稳定，不一定可信的，而期望指长期观察结果，是赌博机的固有属性，稳定又可信。\n**那么，$\\hat{Q}(a)$ 与其期望的“偏差”就是上文霍夫丁不等式中的 $t$。“偏差”越大，就越“值得”探索。**这个“偏差”就是不确定性度量 uncertainty $U(a)$。\n进而，如果我们令不等式右边 exp 那一坨为概率 $p$，可以得知 （以下简称某一臂摇动获得的效益 $\\hat{Q}(a)$ 为“经验效益”，效益的期望为“真实期望”，不确定性度量 uncertainty 为 $u$）：\n真实期望比经验效益还多 $u$，这件事的概率至多为 $p$； 真实期望不超过我们观测到的经验效益 +$u$，这件事的概率至少为 $1-p$； 随着次数 $n$ 越来越多，$p$ 也在越来越小，所以（2）变得越来越可信，即：真实期望有一个上限值，这个上限值就是我们观测到的经验效益 + 不确定性度量 $u$。 上置信界算法便选取期望奖励上界最大的动作。注意这个“期望奖励上界”肯定指真实期望，我们毕竟是要估算赌博机的固有属性，长期期望的。 我们每次选择的是，期望奖励上界最大的摇臂。 接下来就是不太明白为什么 UCB 算法比 $\\epsilon$-gready 算法的累计懊悔还多了。\n","date":"26 September 2025","externalUrl":null,"permalink":"/posts/ai/rl/%E9%9C%8D%E5%A4%AB%E4%B8%81%E4%B8%8D%E7%AD%89%E5%BC%8F%E5%92%8C%E4%B8%8A%E7%95%8C%E7%BD%AE%E4%BF%A1%E7%AE%97%E6%B3%95/","section":"Posts","summary":"","title":"霍夫丁不等式和上界置信算法","type":"posts"},{"content":" 容器操作 # STL 常用容器操作对照表 # 容器 初始化方法 增 (Insert) 删 (Erase) 查 (Find / Access) 改 (Update) 备注 / 底层实现 vector vector\u0026lt;T\u0026gt; v;, vector\u0026lt;T\u0026gt; v(n, val); push_back / insert / emplace pop_back / erase / clear 随机访问 [] / at [] / at 直接修改 动态数组，连续内存，支持随机访问 deque deque\u0026lt;T\u0026gt; d;, deque\u0026lt;T\u0026gt; d(n, val); push_back / push_front / insert / emplace pop_back / pop_front / erase / clear 随机访问 [] / at [] / at 直接修改 分段连续内存，支持两端高效操作 list list\u0026lt;T\u0026gt; l;, list\u0026lt;T\u0026gt; l(n, val); push_back / push_front / insert / emplace pop_back / pop_front / erase / clear 迭代器遍历查找 迭代器指向元素可修改 双向链表，节点分散存储 map map\u0026lt;K,V\u0026gt; m;, map\u0026lt;K,V\u0026gt; m{{k1,v1},{k2,v2}}; insert / emplace / operator[] erase / clear find / count / lower_bound / upper_bound operator[] 修改 value 红黑树，有序唯一键值对 multimap multimap\u0026lt;K,V\u0026gt; m; insert / emplace erase / clear find / count / lower_bound / upper_bound / equal_range ❌ key 不可改，value 可改 红黑树，有序，允许重复 key unordered_map unordered_map\u0026lt;K,V\u0026gt; m; insert / emplace / operator[] erase / clear find / count operator[] 修改 value 哈希表，无序唯一键值对 set set\u0026lt;T\u0026gt; s;, set\u0026lt;T\u0026gt; s{val1, val2}; insert / emplace erase / clear find / count / lower_bound / upper_bound ❌ 不可直接修改元素 红黑树，有序唯一集合 multiset multiset\u0026lt;T\u0026gt; s; insert / emplace erase / clear find / count / lower_bound / upper_bound / equal_range ❌ 不可直接修改元素 红黑树，有序，允许重复元素 unordered_set unordered_set\u0026lt;T\u0026gt; s; insert / emplace erase / clear find / count ❌ 不可直接修改元素 哈希表，无序唯一集合 stack stack\u0026lt;T\u0026gt; s;, stack\u0026lt;T, deque\u0026lt;T\u0026gt;\u0026gt; s; push / emplace pop top top() 修改 适配器，默认基于 deque 实现 queue queue\u0026lt;T\u0026gt; q;, queue\u0026lt;T, deque\u0026lt;T\u0026gt;\u0026gt; q; push / emplace pop front / back front() / back() 修改 适配器，默认基于 deque 实现 priority_queue priority_queue\u0026lt;T\u0026gt; pq;, priority_queue\u0026lt;T, vector\u0026lt;T\u0026gt;, less\u0026lt;T\u0026gt;\u0026gt; pq; push / emplace pop top ❌ 不能直接修改元素 适配器，默认基于 vector + 堆 (heap) 实现 Python 常用数据结构操作对照表 # 容器 初始化方法 增 (Insert) 删 (Erase) 查 (Find / Access) 改 (Update) 备注 / 底层实现 list lst = [], lst = [val1, val2] append / insert / extend pop / remove / clear 下标 [] / index / 遍历 [] 直接修改 动态数组（类似 C++ vector），支持随机访问 deque (collections.deque) from collections import deque; d = deque() append / appendleft / extend / extendleft pop / popleft / remove / clear 下标 []（部分实现支持 O(1)）/ 遍历 [] 修改元素 双端队列，基于双向链表/块状数组，适合两端操作 set s = set(), s = {val1, val2} add / update remove / discard / pop / clear in 成员检测 ❌ 不可直接修改元素 哈希表实现，无序唯一集合 frozenset fs = frozenset([val1, val2]) ❌ 不可添加 ❌ 不可删除 in 成员检测 ❌ 不可修改 不可变集合，哈希表实现 dict d = {}, d = {k1:v1, k2:v2} dict[key] = value / update / setdefault pop(key) / popitem / clear / del dict[key] / get / in dict[key] = new_value 哈希表实现，无序（Python 3.7+ 保持插入顺序） defaultdict (collections) from collections import defaultdict; d = defaultdict(type) defaultdict[type] 自动插入 同 dict 同 dict 同 dict dict 的子类，缺省值自动生成 OrderedDict (collections) from collections import OrderedDict; od = OrderedDict() update / move_to_end pop / popitem / clear 同 dict 同 dict dict 的子类，记录插入顺序（Python 3.7+ 已默认） Counter (collections) from collections import Counter; c = Counter() update subtract / clear counter[key] / elements / most_common counter[key] = new_val 计数器，本质是 dict 的子类 heapq (最小堆) import heapq; h = [] heappush heappop / heapreplace / heappushpop 最小值 heap[0] ❌ 不可直接修改元素（需 pop+push） 基于最小堆，默认最小值优先 queue.Queue from queue import Queue; q = Queue() put get queue[0] (不推荐) / get_nowait ❌ 不能直接改 线程安全队列，基于锁实现 LifoQueue from queue import LifoQueue; q = LifoQueue() put get queue[-1] (不推荐) ❌ 栈结构，线程安全 PriorityQueue from queue import PriorityQueue; pq = PriorityQueue() put (存 (priority, item)) get 最小优先级元素 ❌ 基于 heapq 实现，线程安全 Go 常用数据结构操作对照表 # 容器 初始化方法 增 (Insert) 删 (Erase) 查 (Find / Access) 改 (Update) 备注 / 底层实现 slice s := []T{}, s := make([]T, len, cap) append / copy 手动截断（如 s = append(s[:i], s[i+1:]…)） 下标 s[i] s[i] = val 动态数组，底层基于数组 + 容量自动扩容（类似 C++ vector / Python list） array var a [n]T, a := [n]T{val1, val2} 固定长度，初始化时赋值 ❌ 长度固定不可删 下标 a[i] a[i] = val 固定长度数组，值类型 map m := map[K]V{}, m := make(map[K]V) m[key] = value delete(m, key) m[key] / value, ok := m[key] m[key] = newValue 哈希表实现，键唯一，无序（Go 1.12+ 随机迭代顺序） struct var s T, s := T{field1: val1} 直接定义字段赋值 ❌ 无直接删除（可设零值） s.field s.field = val 自定义复合数据类型，类似 C 结构体 list (container/list) import \u0026quot;container/list\u0026quot;; l := list.New() PushBack / PushFront / InsertAfter / InsertBefore Remove(element) 遍历迭代器 Front() / Back() / Next() element.Value = newVal 双向链表实现，元素非连续存储 heap (container/heap) import \u0026quot;container/heap\u0026quot;; h := \u0026amp;HeapType{} heap.Push heap.Pop h[0] 最小元素 ❌ 不可直接修改（需 Pop+Push） 基于最小堆，需要实现 heap.Interface ring (container/ring) import \u0026quot;container/ring\u0026quot;; r := ring.New(n) r.Link 连接环 r.Unlink(n) r.Value / r.Next() / r.Prev() r.Value = val 环形链表，适合循环队列 sync.Map import \u0026quot;sync\u0026quot;; var m sync.Map Store(key, val) Delete(key) Load(key) / LoadOrStore Store(key, newVal) 并发安全哈希表，适合多线程场景 chan (channel) ch := make(chan T), ch := make(chan T, cap) ch \u0026lt;val 发送 ❌ 无删除，需 close(ch) \u0026lt;-ch 接收 ❌ 不支持修改 CSP 并发模型，底层队列，线程安全 ","date":"20 August 2025","externalUrl":null,"permalink":"/posts/dev/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AE%B9%E5%99%A8%E6%93%8D%E4%BD%9C/","section":"Posts","summary":"","title":"数据结构容器操作","type":"posts"},{"content":"","date":"13 August 2025","externalUrl":null,"permalink":"/tags/flow/","section":"Tags","summary":"","title":"Flow","type":"tags"},{"content":" Flow Matching # 参考资料：\n知乎-深入解析Flow Matching技术 回忆的话看这个就够了 下面的视频推导得也很详细：\n这个 flow-matching 网上的讲解似乎不止一个版本，李宏毅老师讲了一个基于反函数的 flow-matching (OpenAI GLOW, 2018) 算法。本篇的内容相较于 GLOW 是比较新的，基于论文 [2210.02747] Flow Matching for Generative Modeling，这个算法也是 Stable Diffusion 3 的基础。\nPreliminary # Flow matching 的基本假设很符合直觉，建立关键概念的数学直觉，远比推导本身要更重要。\nFlow 的数学直觉 # 在 flow matching 中，$x \\sim p_{t}(x)$ 是从分布 $p_{t}(x)$ 里采样得到的数据。我们假设有一个随时间 $t \\in [0,1]$ 变化的过程，可以一步一步地把一个混沌初开的初始分布 $p_{0}(x)$ 变化为我们想要的目标分布 $p_{1}(x)$。那么我们在初始分布时采样的数据就是 $x_{0} \\sim p_{0}(x)$，在目标分布采样的数据就是 $x_{1} \\sim p_{1}(x)$。\n$p_{0}(x)$ 不一定为高斯分布，$p_{1}(x)$ 也不一定训练数据的分布 注意这里我并没有把初始分布假设为高斯分布，原文也没有，因为它其实可以为很多种分布——只是推导到最后发现高斯分布是一种最好计算的情况。\n至于 $p_{1}(x)$，也并不一定是训练数据。原文和后文中训练数据的分布是 $q(x_{1})$，而我们的目的就是要让最后的分布 $p_{1}(x_{1})$ 尽可能逼近 $q(x_{1})$。\n所以我们的 $x_{t}$、$p_{t}(x)$ 都是会随着时间 $t$ 变化的，都是在朝着尽可能地从一个任意分布到目标分布的方向去变化。那么我们就可以定义 flow 和向量场 $v$ 的概念：\n$$ \\frac{d \\phi_{t}(x_{t})}{dt}=v_{\\theta}(x_{t},t) $$$x_{t}$ 就是粒子群，我们的 flow 就是 $\\phi_{t}$，是一个描述 $x_{t}$ 空间上变化的函数，初始的一群粒子 $x_{0}$ 在向量场 $v_{\\theta}$ 的推动下，自发朝着目标 $x_{1}$ 移动，这个过程仿佛是河流里的一堆水分子在重力势能的作用下往低处流一样。\n向量场 $v_{\\theta}$，是 flow 对时间 $t$ 的导函数，反映的是 flow 作用下的粒子群，在下一个时间里往哪里走。而应该注意到它带了 $\\theta$，就说明 $v_{\\theta}$ 其实是我们要拟合的一个目标。真实世界里粒子运动的向量场 Ground Truth $u_{t}$ 我们不知道诶，所以要用神经网络 $v_{\\theta }$ 拟合 $u_{t}$。\n这么想清楚了的话，损失函数便呼之欲出：\n$$ \\mathcal{L}_{FM}=\\mathbb{E}_{t,p_{t}(x)}[||v_{\\theta,t}-u_{t}||^2_{2}] \\tag 1 $$仔细观察之，损失函数表示求一个 L2 损失，在什么范围内求呢？在所有时间尺度 $t \\in [0,1]$ 和所有概率路径 $p_{t}(x)$ 上！说实话，我们的粒子流 $x_{0}$ 运动的路径千千万，既可以走直线直接到 $x_{1}$（这里埋个伏笔），也可以歪七扭八地走到 $x_{1}$，甚至还有可能到不了 $x_{1}$，我怎么能够在所有概率路径上求积分呢？这就是论文要解决的问题咯。\nConditional Flow Matching # 按直觉来说，$p_{t}(x)$ 代表 $x_{0} \\to x_{1}$ 的概率路径千千万，但弱水三千，若是只取一瓢饮，不就很好嘛？\n那我们取最终能到达 $x_{1}$ 的那条。那你可能会说 $x_{0}$ 还能走不到 $x_{1}$？还真有可能，谁规定山顶流下的水流就一定能到达最终的目的地？\n数学表达式是这样的：\n$$ p_{1}(x)=\\int p_{1}(x|x_{1})q(x_{1})dx_{1} \\approx q(x) $$$p_{1}(x)$ 是 $t=1$ 时刻的分布，$q(x_{1})$ 是训练数据集在最后粒子群 $x_{1}$ 上的分布，我们想要 $p_{1}(x)$ 最终逼近 $q(x)$ 也就是训练数据集上的分布。\n论文花了很大篇幅证明，在 假设一个 condition：$x_{0}$ 能到达 $x_{1}$ 的概率路径上计算的损失函数 (2)，相比对所有概率路径求损失函数 (1)，算出的梯度是一样的。这个方法在论文中叫Conditional Flow Matching (CFM)\n$$ \\mathcal{L}_{cfm}=\\mathbb{E}_{t,q(x_{1}),p_{t}(x|x_{1})}[||v_{\\theta,t}(x)-u_{t}(x)||_{2}^2] \\tag 2 $$ Optimal Transport Conditional VFs # 论文其实还证了很多东西，如果要完整理解的话可以看开头提到的视频。我觉得推了这么多结果用的最多的其实是最简单的一种情况，所以没必要事无巨细地去理解。\n这个最简单的情况就是 Optimal Transport，它是说粒子群从 $x_{0} \\to x_{1}$，最优路径肯定是直线过去，即：\n$$ x_{t}​=(1−t)x_{0}+tx_{1}​ $$也就是 $x_{t}$ 是 $x_{0}$ 与 $x_{1}$ 之间的线性差值。它的速度场和 $x_{t}$ 无关：\n$$ u_{t}=\\frac{dx_{t}}{dt}=x_{1}-x_{0} $$损失函数：\n$$ \\mathcal{L}_{cfm}=\\mathbb{E}_{t,q(x_{1}),p_{t}(x|x_{1})}[||v_{\\theta,t}(x)-u_{t}(x)||_{2}^2]=\\mathbb{E}_{t,q(x_{1}),p_{t}(x|x_{1})}[||v_{\\theta}(x,t)+x_{0}-x_{1}||_{2}^2] $$完事儿之后就可以训练了！\nTraining # 训练过程：\nfor all $x_{1}$ in training dataset: $t \\sim \\text{Uniform}(0,1)$ 采样得到得到一个时间步 $t$ $x_{0} \\sim N(0,1)$ 采样得到高斯噪声作为起始粒子群 $x_{0}$ $x_{t}=(1-t)x_{0}+tx_{1}$ 计算出 $t$ 时刻的粒子群 $x_{t}$ $u_{t}=x_{1}-x_{0}$ 计算向量场 $v_{t}(x_{t},t)$ 为 $t$ 时刻模型预测的向量场 $v$ 计算 $\\text{MSE}(v_{t},u_{t})$，梯度下降训练模型 简单写一个训练代码。网络结构就不写了\ndef cfm_loss(model, x1): B = x1.size(0) # Batch size of training data x1 t = torch.rand(B, device=x1.device) # U[0,1] x0 = torch.randn_like(x1) # 高斯先验 xt = (1 - t[:, None, None, None]) * x0 + t[:, None, None, None] * x1 ut = x1 - x0 # 真实速度（线性插值的导数） v = model(xt, t) return F.mse_loss(v, ut) for epoch in range(20): model.train() for x, _ in loader: x = x.to(device) loss = cfm_loss(model, x) opt.zero_grad(); loss.backward(); opt.step() print(f\u0026#34;epoch {epoch:02d} | loss {loss.item():f}\u0026#34;) 这里的细节 $t$ 是取 B 个服从 Uniform(0,1) 的数，表示均匀时间步。\nSampling # 采样的核心公式是欧拉积分\n$$ x_{t+dt}=x_{t}+v_{\\theta}(x_{t},t)*dt $$ Euler Method： # Initialize $x_0 \\sim \\mathcal{N}(0,1)$ . Set T (number of steps) and compute $dt = \\frac{1}{T}$ . For $i = 0, 1, \\ldots, T-1$ : Compute time $t_i = i \\cdot dt$ . Evaluate velocity field $v_\\theta(x_{t_i}, t_i)$ . Update $x_{t_{i+1}} = x_{t_i} + v_\\theta(x_{t_i}, t_i) \\cdot dt$. Output $x_{T}$ as the final sample. Improved Euler Method (Heun) # The Heun method uses a predictor-corrector approach:\n$$ x_{t+dt} = x_t + \\frac{1}{2} \\cdot dt \\cdot \\big( v_\\theta(x_t, t) + v_\\theta(x_{t+dt}^{\\text{pred}}, t+dt) \\big) $$where $x_{t+dt}^{\\text{pred}} = x_t + v_\\theta(x_t, t) \\cdot dt$.\nSteps:\nInitialize $x_0 \\sim \\mathcal{N}(0,1)$ . Set T (number of steps) and compute $dt = \\frac{1}{T}$ . For $i = 0, 1, \\ldots, T-1$ : Compute time $t_i = i \\cdot dt$ . Evaluate $v_1 = v_\\theta(x_{t_i}, t_i)$ . Compute predictor $x_{t_{i+1}}^{\\text{pred}} = x_{t_i} + v_1 \\cdot dt$ . Evaluate $v_2 = v_\\theta(x_{t_{i+1}}^{\\text{pred}}, t_i + dt)$ . Update $x_{t_{i+1}} = x_{t_i} + \\frac{1}{2} \\cdot dt \\cdot (v_1 + v_2)$ . Output $x_T$ as the final sample. @torch.no_grad() def sample(model, n=16, steps=60): model.eval() x = torch.randn(n, 3, 32, 32, device=device) # t=0 的高斯 t0, t1 = 0.0, 1.0 ts = torch.linspace(t0, t1, steps+1, device=device) # Heun（改进欧拉）稳定些 for i in range(steps): t = ts[i].expand(n) dt = (ts[i+1] - ts[i]).item() v1 = model(x, t) # k1 x_pred = x + dt * v1 v2 = model(x_pred, ts[i+1].expand(n)) # k2 x = x + 0.5 * dt * (v1 + v2) return torch.clamp(x, -1, 1) ","date":"13 August 2025","externalUrl":null,"permalink":"/posts/ai/aigc/cornerstone/flow-matching-%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%90%86%E8%A7%A3/","section":"Posts","summary":"","title":"Flow Matching 的基本理解","type":"posts"},{"content":"","date":"11 August 2025","externalUrl":null,"permalink":"/tags/ddpm/","section":"Tags","summary":"","title":"DDPM","type":"tags"},{"content":" DDPM Key Points # 参考资料 # 生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼 - 科学空间|Scientific Spaces 扩散模型(Diffusion Model)详解：直观理解、数学原理、PyTorch 实现 | 周弈帆的博客 苏神写的无疑是最容易理解的（但不是原文作者的推导方法），周弈帆的博客最贴近原作，还有代码讲解。这应该是我读过的写得最清楚的两篇了。\n这个视频也令人豁然开朗：\n原代码在：GitHub - hojonathanho/diffusion: Denoising Diffusion Probabilistic Models\nDDPM # Perturbation Process # 加噪 Perturbation，是从原图 $x_{0}$ 逐步加噪到高斯噪声 $x_{T}$：\n$$ x_{t}=\\sqrt{ \\beta_{t} } \\epsilon+\\sqrt{ 1-\\beta_{t} } x_{t-1} \\tag{1} $$$\\beta_{t}$ 随时间 $t$ 增长，betas = np.linearspace(0.0001, 0.02, 1000) 也就是从 min=0.0001 到 max=0.02 均匀生成 1000 个 tensor，导致加的噪声越来越大，均值缩放程度越来越小。 $\\alpha_{t}=1-\\beta_{t}$。alphas = 1.0 - betas\n加的噪声 noise = torch.randn_like(x) 与 $x$ 的形状保持一致。\n图像像素 $x$ 是原图 [0,255] 经过映射到 [-1,1] 再加噪的。映射公式是：$x:=\\frac{x}{255}*2-1$。torchvision.transforms.ToTensor() 可以做到把图像从 [0,255] 转化为 [0,1] 的张量。\n我们可以从 $x_{0}$ 直接生成任意时间的图像 $x_{t}$:\n$$ x_{t}=\\sqrt{ 1-\\bar{\\alpha_{t}} }\\epsilon + \\sqrt{ \\bar{\\alpha_{t}} } x_{0} \\tag{2} $$我们训练的核心就是在加噪过程，基本 idea 就是用网络预测一个 $\\epsilon_{\\theta}$ 去逼近加噪过程的噪声 $\\epsilon$。\nDenoise Process # 这个和采样过程关系比较大。\n去噪过程相反。从一张高斯分布的图片 $x_{T}$ 到高清原图 $x_{0}$，完成生成过程。\nDDPM 不是去直接预测原图，而是预测一个可以去除的噪声 $\\epsilon_{\\theta}$，通过以下公式：\n$$ x_{t-1}=\\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha_{t}} }}\\epsilon_{\\theta} \\right)+\\sigma_{t}z \\tag{3} $$关键的就是这个可去除噪声 $\\epsilon_{\\theta}$，由神经网络预测。输入是 $t$ 时刻的图像 $x_{t}$ 和编码的时间 $t$ 信息，经过 UNet 网络预测。$z \\in N(0,1)$\n这个式子在苏剑林的博客 生成扩散模型漫谈（一）：DDPM = 拆楼 + 建楼 - 科学空间|Scientific Spaces 里推导得比较好。他注意到公式 (1) 可以用\n$$ x_{t-1}=\\frac{1}{\\sqrt{ \\alpha_{t} }}x_{t}-\\sqrt{ \\beta_{t} }\\epsilon \\tag 4 $$表示，所以先假设了去噪过程 $x_{t}$ 和 $\\epsilon$ 之前的系数，然后解方程推出来的。\nTraining # 与加噪过程紧密相关，基本 idea 就是用 UNet 网络预测一个 $\\epsilon_{\\theta}$ 去逼近每一个加噪过程的噪声 $\\epsilon$。\n$x_{0} \\sim q(x_{0})$，意思是从数据集中采样一张原图 $x_{0}$ $t \\sim \\text{Uniform}({1,\\dots T})$，Uniform 是均匀分布，意思是选定一个时间步 $t$ $\\epsilon \\sim N(0,1)$，从高斯分布中采样出该时间步 $t$ 加的噪声。这个 $\\epsilon$ 是 Ground Truth 预测该步加噪后的图片 $x_{t}=\\sqrt{ 1-\\bar{\\alpha_{t}} }\\epsilon + \\sqrt{ \\bar{\\alpha_{t}} } x_{0} \\tag{2}$。$\\alpha_{t}$ 是之前通过假设 $\\beta_{t}$ 得出的常数 用 UNet 预测噪声。输入 encoded $t$ 和在第 4 步获得的 $t$ 时刻的图像 $x_{t}$，预测一个噪声 $\\epsilon_{\\theta}(x_{t}, t)=\\epsilon_{\\theta}(\\sqrt{ 1-\\bar{\\alpha_{t}} }\\epsilon + \\sqrt{ \\bar{\\alpha_{t}} } x_{0},t)$。这个 $\\epsilon_{\\theta}$ 是模型预测值，需要逼近 Ground Truth 计算梯度 $\\nabla_{\\theta}|| \\epsilon - \\epsilon_{\\theta}(\\sqrt{ 1-\\bar{\\alpha_{t}} }\\epsilon + \\sqrt{ \\bar{\\alpha_{t}} } x_{0}) ||_{2}^2$ 重复以上过程直至收敛 其实核心思想就在于，我们通过公式可以直接从原图 $x_{0}$ 加一个高斯噪声 $\\epsilon$，到 $x_{t}$，然后我们只需要得知第 $t$ 步加噪多少就可以了——这一部分直接交给神经网络去拟合。如果我们能预测出第 t 步加的噪声，那么我们就能一步一步反向去噪得到原图。这就是采样过程。\n最后这个损失函数推导还挺难的。但是很符合直觉。\nSampling # 说是采样，其实就是推理过程。基本 idea 是我们已经有可以推理出每一个时间 t 下，加了多少噪声的模型 $\\epsilon_{\\theta}(x_{t},t)$ 了，那我们用公式 (3) 反向推回去就行。\n$x_{t}=N(0,1)$。这是推理的起点。在加噪之后获得一张纯高斯噪声图片 $x_{t}$。 for t = T, …, 1 循环以下过程 先采样 $z \\sim N(0,1) \\text{ for t\u0026gt;1 , else }z=0$。这个是公式 (3) 里反向去噪用到的高斯 perturbation $x_{t-1}=\\frac{1}{\\sqrt{ \\alpha_{t} }}\\left( x_{t}-\\frac{1-\\alpha_{t}}{\\sqrt{ 1-\\bar{\\alpha_{t}} }}\\epsilon_{\\theta}(x_{t},t) \\right)+\\sigma_{t}z$，利用公式 (3) 递推去噪的图片 返回去噪的图片 $x_{0}$ 再次提醒我们的 UNet 网络预测的是噪声，输入是图像 $x_{t}$ 以及编码的时间信息 $t$，输出是 $\\epsilon_{\\theta}(x_{t},t)$\n","date":"11 August 2025","externalUrl":null,"permalink":"/posts/ai/aigc/cornerstone/diffusion-model/","section":"Posts","summary":"","title":"Diffusion Model","type":"posts"},{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/tags/langevin/","section":"Tags","summary":"","title":"Langevin","type":"tags"},{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/tags/mcmc/","section":"Tags","summary":"","title":"MCMC","type":"tags"},{"content":" MCMC # 参考： https://zhuanlan.zhihu.com/p/37121528 https://medium.com/@WeiranLin/acceptance-rejection-sampling-method-illustrated-with-python-0552c0eb27d2 https://chatgpt.com/share/688ce531-d530-8011-9afc-b76d072f1215\n本文假设你已经了解了马尔可夫链的基本定义、平稳分布的含义。\n动机 # 我在学这个算法之前，参考了许多网上的教程，结果上来就是数学公式，看到一半还不知道自己在解决什么样的问题。这部分把要解决什么样的问题这件事情说清楚。\n我们要知道，现实世界中的很多概率分布是不太好估计的，并非所有概率分布都长得像高斯分布那样优雅。比如下面的分布：\nMade by Geogebra https://www.geogebra.org/calculator/qfjrjj6v. Example from FunInCode, Bilibili\n绿色线这个关于 $x$ 的分布，就问你不看表达式的情况下怎么求它的分布 $P(X)$？求不了一点吧。\n上面这种纯恶心人的分布还是一维的，更别提现实生活中更复杂的分布，例如生成式模型任务让你求一张图片 256 个像素点的联合分布 $P(x_1,x_2,…,x_{256})$ 🤮。我们不得不找一种“用数据去估计分布”的方法。\n但其实，我们也没有必要把分布的函数形式给求出来。很多分布是没有好看的函数形式的，也可能根本求不出来。我们想要的只是“从分布中采样”这么一件事情。我们可以不知道上面绿色这个分布的形式，但却可以从绿色这个分布中 sample 出很多样本 $x_1,x_2,…,x_n$。这就很 nice！以生成式模型为例，假设我们在不知道“麦晓雯美图”分布是什么的情况下，依然可以从“麦晓雯美图”分布中采样出符合这个分布的 256 个像素点 $(x_1,x_2,…,x_{256})$，那这个生成式任务不就完成了吗？\nMCMC 采样 # 平稳分布、详细平衡条件 # 那么先哲们是如何通过马尔科夫链，去跟采样这件事情联想起来的呢？\n其实马尔可夫链本身就可以看作是对概率分布 $P$ 的建模。举个很简单的例子，我们要对北京 A、上海 B三个地方的人口密度分布进行建模。当前的概率分布为 $\\pi(A)=0.6, \\pi(B)=0.4$，并假设人口在城市间的转移概率服从马尔可夫链：\n当前城市 → 下一个城市 A B A 0.7 0.3 B 0.5 0.5 那我们想问：下一步 A 点的分布是什么？\n好算！$π(A)=π(A)⋅P(A→A)+π(B)⋅P(B→A)=0.6⋅0.7+0.4⋅0.5=0.42+0.2=0.62$\n原先的分布里 A 的概率是 0.6，现在变成了 0.62，依然不平稳。但根据遍历定理，再过几步，A 和 B 的概率分布一定可以达到平稳状态。\n从上面的例子中我们提出几个符号：\n$\\pi(x)$：表示当前在状态 x 的概率（这是你当前的分布） $P(x \\to x\u0026rsquo;)$：从状态 x 跳到状态 x′ 的转移概率 所以 $\\pi(x) P(x \\to x\u0026rsquo;)$：表示你现在在 x，然后走到 x′的整体概率流量。遍历所有能走到 $x\u0026rsquo;$ 的路，就是 $\\Sigma \\pi(x)P(x \\to x\u0026rsquo;) ,\\forall x$ ，这个式子表示下一个时间步里 $x\u0026rsquo;$ 的分布情况，如果能够与 $\\pi(x\u0026rsquo;)$——也就是当前 x’的分布相等，那么说明什么？\n说明“再走一步，分布不变”，说明我们达到了平稳分布！\n而详细平衡条件是一个更强的条件，能推出平稳分布。如果我们有从北京到上海的人流，与从上海到北京到人流相等，——也就是任意两个点之间双向的流量相等，——那么就说明系统达到平稳分布了。\n$$ \\pi(x)P(x \\to x')=\\pi(x')P(x'\\to x) $$以上就是详细平衡条件。\n从前面那个公式：\n$π(x\u0026rsquo;)=∑xπ(x)P(x→x\u0026rsquo;)\\pi(x\u0026rsquo;) = \\sum{x} \\pi(x) P(x \\to x\u0026rsquo;)$\n如果每一对 $(x, x\u0026rsquo;)$ 都满足 $\\pi(x) P(x \\to x\u0026rsquo;) = \\pi(x\u0026rsquo;) P(x\u0026rsquo; \\to x)$\n那么就可以替换右边成：\n$\\sum_{x} \\pi(x\u0026rsquo;) P(x\u0026rsquo; \\to x) = \\pi(x\u0026rsquo;) \\sum_{x} P(x\u0026rsquo; \\to x) = \\pi(x\u0026rsquo;)$（因为对马尔科夫链来说，$\\sum_{x} P(x\u0026rsquo; \\to x) = 1$）\n所以，详细平衡 ⇒ 平稳分布。\nM-H MCMC # MH 的厉害之处就是：\n通过一个简单的“接受 - 拒绝”机制，构造出满足详细平衡的马尔科夫链，从而让我们能从目标分布 $\\pi(x)$ 采样。\n基本概念有：\n🎯目标分布 $\\pi(x)$。我们的目标分布 ${\\pi(x)}$ 可以是一个很复杂的分布，不需要知道归一化常数 $Z$。 ❔提议分布 $q(x\u0026rsquo;|x)$。表示当前状态 x 下，打算怎么样跳到 x’。 有了提议分布，我们却并不打算接受它。我们以概率 $\\alpha=min(1, \\frac{\\pi(x\u0026rsquo;)q(x|x\u0026rsquo;)}{\\pi(x)q(x\u0026rsquo;|x)})$ 的概率接受它。 很懵，我来告诉你为什么这么做。\n现在我们的转移概率变成了 $P(x \\to x\u0026rsquo;)=q(x\u0026rsquo;|x)\\alpha$，乘上当前的状态 $\\pi(x)$ 就是下一个时刻从 x 到 x’的“流量”：\n$π(x)P(x→x\u0026rsquo;)=π(x)q(x\u0026rsquo;∣x)α(x,x\u0026rsquo;)=min(π(x)q(x\u0026rsquo;∣x),π(x\u0026rsquo;)q(x∣x\u0026rsquo;))$\n观察到这个式子是对称的！也就是说下一个时刻从 x’到 x 的“流量”，依然等于右式。想想之前“详细平稳”的条件——任意两个点之间双向的流量相等，我们成功证明了按照 M-H 方法，系统达到了详细平衡，也达到了平稳分布。\nLangevin MCMC # 人工智能专业的学生表示 Langevin 方程懂不了一点。我直接复制 GPT 了：\n朗之万动力学（Langevin Dynamics）采样是非常有趣且强大的采样方法，尤其适合于连续分布和物理模拟。我们可以将它视为一种基于物理系统的马尔科夫链蒙特卡洛（MCMC）方法，它使用了梯度信息来引导采样，从而能够更加高效地在复杂分布中采样。\n🧠 朗之万动力学采样的基本思路 # 朗之万动力学采样（Langevin sampling）是通过模拟物理系统中的粒子运动来获得目标分布的样本。这个过程受到以下两方面的影响：\n势能（Potential energy）：它是目标分布的对数值，用来定义粒子的行为。 噪声：模拟粒子的随机运动，用来引入随机性。 这类似于温度控制的物理过程，其中粒子在潜在势场中受力，并且每次迭代都受到随机扰动。\n🔬 朗之万方程（Langevin Equation） # 在经典物理中，朗之万方程描述了粒子如何在势能场中运动。其数学表达式如下：\n$$ \\frac{dx}{dt} = - \\nabla U(x) + \\sqrt{2 \\gamma} \\, \\eta(t) $$其中：\nx 是粒子的状态（类似于 MCMC 中的采样点）， U(x) 是势能（在采样中，它通常是目标分布的负对数）， γ 是摩擦系数（决定粒子运动的阻力）， η(t) 是一个高斯白噪声，代表随机扰动。 🧑‍🔬 离散化的朗之万动力学采样 # 由于朗之万方程是连续的，我们通常需要将其离散化以便在计算机上实现。在离散化时，我们使用欧拉方法：\n$$ x_{t+1}=x_t−ϵ∇U(x_t)+2ϵγ ξx_{t+1} = x_t - \\epsilon \\nabla U(x_t) + \\sqrt{2 \\epsilon \\gamma} \\, \\xi_t $$其中：\n$\\epsilon$ 是步长（控制每一步的更新幅度）， $\\nabla U(x_t)$ 是目标分布（势能）的梯度，似乎只有这个是比较重要的 $\\xi_t$ 是标准正态分布的随机噪声（每一步的随机扰动）。 在 Energy-based model 的语境下：\n$$ p(x)=\\frac{e^{-U(x)}}{Z} $$能量\n$$ U(x)=-\\log p(x) $$我们已知条件是 score-function\n$$ s_\\theta(x) \\approx \\nabla_x \\log p(x) $$因此在 Langevin MCMC 中\n$$ \\nabla_x U(x_t)=-\\nabla_x \\log p(x) $$对应了“概率往梯度上升的方向走”。\n🎯 步骤概述：朗之万采样算法 # 初始化：选择一个起始点 $x_0$。 计算梯度：计算目标分布的梯度 $\\nabla U(x_t)$。 更新状态：根据离散化的朗之万方程更新状态 $x_{t+1}$。 重复：重复多次，直到收集足够的样本。 🧑‍💻 代码实现（伪代码示例） # import numpy as np import matplotlib.pyplot as plt ## 目标分布：标准正态分布的负对数 def target_distribution(x): return 0.5 * x**2 ## U(x) = (1/2) * x^2 ## 梯度：目标分布的梯度 def grad_target_distribution(x): return x ## U\u0026#39;(x) = x ## 朗之万动力学采样 def langevin_dynamics(n_samples, step_size=0.1, gamma=0.1): ## 初始状态 x = 0 samples = [x] for _ in range(n_samples - 1): ## 计算梯度 grad_U = grad_target_distribution(x) ## 添加噪声（高斯扰动） noise = np.random.normal(0, np.sqrt(2 * step_size * gamma)) ## 更新状态 x = x - step_size * grad_U + noise ## 记录样本 samples.append(x) return np.array(samples) ## 采样 10000 次 samples = langevin_dynamics(10000) ## 绘制直方图 plt.hist(samples, bins=50, density=True, alpha=0.6, color=\u0026#39;b\u0026#39;) x = np.linspace(-4, 4, 100) plt.plot(x, np.exp(-target_distribution(x)), \u0026#39;r\u0026#39;, lw=2) ## 目标分布的密度函数 plt.title(\u0026#39;Langevin Dynamics Sampling\u0026#39;) plt.show() 📚 解释代码： # 目标分布：我们选了一个简单的标准正态分布的负对数（势能）作为目标分布 $U(x)。$ 这其实是 $\\pi(x) = \\mathcal{N}(0, 1)$ 的负对数。 梯度计算：计算 $U(x)$ 的梯度（对于 $U(x) = \\frac{1}{2} x^2$ 来说，梯度就是 $\\nabla U(x) = x$）。 更新状态：每次更新状态时，除了梯度项外，还加了一个来自高斯噪声的扰动项，确保状态更新是随机的。 绘制直方图：运行采样并绘制直方图，看看朗之万采样生成的样本是否接近目标分布。 🎯 优点与应用 # 高效采样：朗之万采样可以高效地在复杂的目标分布中采样，尤其适用于有梯度信息的情形。 物理模型模拟：广泛应用于分子动力学模拟、粒子系统、统计物理等领域。 与 MH 对比：相较于 Metropolis-Hastings，朗之万采样通过使用梯度信息，可以在某些情况下更快收敛，并且更好地探索复杂的多峰分布。 ","date":"5 August 2025","externalUrl":null,"permalink":"/posts/ai/aigc/cornerstone/mcmc-markov-chain-monte-carlo/","section":"Posts","summary":"","title":"MCMC (Markov Chain Monte Carlo)","type":"posts"},{"content":"","date":"5 August 2025","externalUrl":null,"permalink":"/tags/dev/","section":"Tags","summary":"","title":"Dev","type":"tags"},{"content":" 训练常用命令 # Scp # 好处在于 windows 上也自带这个命令，而且很简单，只需记住 -r 参数就可以了。\n坏处是小文件特别多的话比较慢。这种情况下建议主用 rysnc 。\n为什么最好不要用 git 呢？首先自己写的 private 仓库代码用 git 不好在服务器上同步，其次 git lfs 也会占用大空间（ du -h ./ 查看），再者部署用 git 也实在不优雅。一般部署是用 scp 传二进制或者 rsync 传 python 这种零散脚本。\n递归复制目录：\nscp -r my_directory user@remote_host:/home/user/directory 复制本地的文件到远程。反之从远程到本地的话就改一改。\n不需要对文件在远程再命名 ( •̀ ω •́ ) 是直接就放进对应目录里\nscp -P 35394 *pt user@remote.com:/root/models/ ## 本地的*.pt文件，注意不带/的话会直接重命名为models！ scp -P 35394 model1.pt model2.pt user@remote.com:/root/models/ ## 可以拷贝多个文件 常用参数 rpvC ：\nr：递归复制整个目录。 P：指定远程主机的 SSH 端口号（默认是 22）。注意， P 是大写 p：保留文件的修改时间、访问时间和权限。 v：显示详细的调试信息，有助于排查问题。 C：启用压缩，可以加快传输速度。 Rsync # 这个和 scp 命令起到的作用差不多，但是支持断点续传、增量传输。但是这个命令 windows 上没有。\n如果 ssh 命令有附加的参数，则必须使用 -e 参数指定所要执行的 SSH 命令。\n最终版命令 # 一般最常用的选项组合 -avzP 来进行传输。支持同步增量传输、改端口、忽略隐藏文件：\nrsync -avzP -e \u0026#39;ssh -p 2234\u0026#39; --exclude=\u0026#39;.*\u0026#39; source/ user@remote_host:/destination 如果要排除一些文件夹（比如说 .venv ），用 --exclude 。\n这里有个坑 路径是自动相对于传输目录的正则，下面的例子如果写 --exclude='./models 就会完全匹配不上。\n## 支持通配符，用于删除mac finder下一些不可名状的小东西和.venv之伦 rsync -avzP --exclude=\u0026#39;.*\u0026#39; --exlude=\u0026#39;models*\u0026#39; /local/path/ user@remote:/remote/path/ 参数 # a、-archive 参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。 -append 参数指定文件接着上次中断的地方，继续传输。 -append-verify 参数跟 -append 参数类似，但会对传输完成后的文件进行一次校验。如果校验失败，将重新发送整个文件。 b、-backup 参数指定在删除或更新目标目录已经存在的文件时，将该文件更名后进行备份，默认行为是删除。更名规则是添加由 -suffix 参数指定的文件后缀名，默认是 ~。 -backup-dir 参数指定文件备份时存放的目录，比如 -backup-dir=/path/to/backups。 -bwlimit 参数指定带宽限制，默认单位是 KB/s，比如 -bwlimit=100。 c、-checksum 参数改变 rsync 的校验方式。默认情况下，rsync 只检查文件的大小和最后修改日期是否发生变化，如果发生变化，就重新传输；使用这个参数以后，则通过判断文件内容的校验和，决定是否重新传输。 -delete 参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。（有风险！模型训练完，如果加这个参数的话直接没了） e 参数指定使用 SSH 协议传输数据。 -exclude 参数指定排除不进行同步的文件，比如 -exclude=\u0026quot;*.iso\u0026quot;。 -exclude-from 参数指定一个本地文件，里面是需要排除的文件模式，每个模式一行。 -existing、-ignore-non-existing 参数表示不同步目标目录中不存在的文件和目录。 h 参数表示以人类可读的格式输出。 h、-help 参数返回帮助信息。 i 参数表示输出源目录与目标目录之间文件差异的详细情况。 -ignore-existing 参数表示只要该文件在目标目录中已经存在，就跳过去，不再同步这些文件。 -include 参数指定同步时要包括的文件，一般与 -exclude 结合使用。 -link-dest 参数指定增量备份的基准目录。 m 参数指定不同步空目录。 -max-size 参数设置传输的最大文件的大小限制，比如不超过 200KB（-max-size='200k'）。 -min-size 参数设置传输的最小文件的大小限制，比如不小于 10KB（-min-size=10k）。 n 参数或 -dry-run 参数模拟将要执行的操作，而并不真的执行。配合 v 参数使用，可以看到哪些内容会被同步过去。 P 参数是 -progress 和 -partial 这两个参数的结合。 -partial 参数允许恢复中断的传输。不使用该参数时，rsync 会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与 -append 或 -append-verify 配合使用。 -partial-dir 参数指定将传输到一半的文件保存到一个临时目录，比如 -partial-dir=.rsync-partial。一般需要与 -append 或 -append-verify 配合使用。 -progress 参数表示显示进展。 r 参数表示递归，即包含子目录。 -remove-source-files 参数表示传输成功后，删除发送方的文件。 -size-only 参数表示只同步大小有变化的文件，不考虑文件修改时间的差异。 -suffix 参数指定文件名备份时，对文件名添加的后缀，默认是 ~。 u、-update 参数表示同步时跳过目标目录中修改时间更新的文件，即不同步这些有更新的时间戳的文件。 v 参数表示输出细节。vv 表示输出更详细的信息，vvv 表示输出最详细的信息。 -version 参数返回 rsync 的版本。 z 参数指定同步时压缩数据。 --delete 删除本机没有，远程有的文件。 Tmux # https://www.ruanyifeng.com/blog/2019/10/tmux.html\n极简流程 # 新建会话 tmux new -s my_session。或者直接 tmux 进入一个从 0 开始自然编号的 session。 在 Tmux 窗口运行所需的程序。 按下快捷键 Ctrl+b d 将会话分离。 下次使用时，重新连接到会话 tmux a -t my_session 或者 tmux attach-session -t my_session。或者直接 tmux a。 支持鼠标滚动 # ctrl+b 进入命令面板，在命令行处输入 : ，然后输入 set -g mouse on 按回车即可。但是我感觉 Ctrl+b [：进入 vi 模式会更舒适一点。\n保存输出 # tmux capture-pane -p -S - -E - \u0026gt; output.txt S -：表示从缓冲区的最开始（最顶端）开始捕获。 E -：表示捕获到缓冲区的末尾（最底端）。如果输出太多，且缓冲区设置有限，早期的输出会被丢弃，不会在这里捕获到。 p：打印捕获内容到标准输出，配合重定向保存文件。 使用 tmux set-option -g history-limit 50000 把缓存区历史记录增大到 50000 条\nSwitch Sessions (Collaborated with nvim) # ctrl+b, s quickly switch between opened sessions.\ntmux 里使用 nvim 似乎会有颜色和 nerd-font 的偏差…解决办法是让 tmux 把 24 位色透传给 Neovim。在 ~/.tmux.conf 里：\nset -g default-terminal \u0026#34;tmux-256color\u0026#34; # 不要写 screen-256color set -as terminal-features \u0026#39;,*:RGB\u0026#39; # 2.9+ 用 RGB；老版本写 \u0026#39;,*:Tc\u0026#39; set -g window-active-style \u0026#39;fg=default,bg=default\u0026#39; 不想折腾了，就不要在 tmux 里开 nvim 就行。在主程序里使用 nvim，然后 ctrl z 进入 bg，再去 tmux 里执行命令，最后回到前台用 fg 命令就行。\n其他常用命令\u0026amp;快捷键 # tmux 快速进入一个 session。从 0 开始编号。 tmux ls 列出所有 session 终止当前窗格的命令： ctrl+b ， x Ctrl+b c：创建一个新窗口，状态栏会显示多个窗口的信息。 Ctrl+b p：切换到上一个窗口（按照状态栏上的顺序）。 Ctrl+b n：切换到下一个窗口。 Ctrl+b \u0026lt;number\u0026gt;：切换到指定编号的窗口，其中的 \u0026lt;number\u0026gt; 是状态栏上的窗口编号。 Ctrl+b w：从列表中选择窗口。 Ctrl+b ,：窗口重命名。 Tee # 作用\u0026amp;需求 # 👉 把标准输入的内容同时输出到屏幕（stdout）和文件。\n对于那种初级的、跑一次就不用管的、但是要持久化的命令——比如在服务器上起一个后端，后续只需要看看 log 的服务，用 nohup 就可以最简单地解决，只需要时不时登上去用 tail 看看跑的怎么样、用 scp 下载 log debug 就行。\n但是深度学习的需求是需要长时间盯着屏幕看输出，又还不能一直看（没那么多时间），跑完又得立刻关机，这时候最好的解决方式就是用 tmux 加 tee。\n用法 # 最好是 tmux 起一个 session 之后：\ncommand | tee out.log 这样 tmux 里也能看到输出，同时输出还会被 file.log 记录下来。（此时要点名 tqdm 库，它的进度条输出到文件的样子又臭又长，但是从终端里看的效果就会好很多）。\ntee 只会重定向标准输出 stdout，而不会重定向错误输出 stderr 。而 python logging 用的是 stderr，所以要是想看到日志，需要这么写：\ncommand 2\u0026gt;\u0026amp;1 | tee out.log 运行完 shutdown 的写法：\n# 让管道返回左边命令的退出码 set -o pipefail command 2\u0026gt;\u0026amp;1 | tee train.log \u0026amp;\u0026amp; shutdown 下面的写法不对！ command | tee train.log \u0026amp;\u0026amp; shutdown command | tee train.log ; shutdown 因为 command | tee train.log \u0026amp;\u0026amp; shutdown 里的 \u0026amp;\u0026amp; 只看管道命令的退出码，而 管道命令的退出码默认由最右边的进程决定。 tee 本身几乎永远返回 0，所以即使 command 里的 Python 抛了异常，整条管道仍然被 shell 视为“成功”，shutdown 就被执行了。\nLn # 创建链接。\n一般用 rsync 命令把代码文件传到 ~ 目录下，然后用 scp 或 jupyter-lab 或 oss 或其他随便什么东西，把模型文件传到 /hy-tmp 下，最后用软链接结合它们。\n比如恒源云， /hy-tmp 文件夹是 SSD，读写速度最快，那么就需要把当前文件夹下的数据集先移动到 /hy-tmp 再创建软连接：\nmv -p ./dataset /hy-tmp/niyuta/dataset \u0026amp;\u0026amp; ln -s /hy-tmp/niyuta/dataset dataset 这里也有坑 如果后面 ln 写的是 ln -s /hy-tmp/niyuta/dataset ./dataset 就会报错本地没有这个文件夹。其实仔细想想就能明白， dataset 是一个符号而已，将其指派为文件夹显然不对。\nTar # 解压 # tar -xvzf file.tar.gz -x 解包（extract） -v 显示过程（verbose，可省略） -z 经过 gzip 压缩 -f 指定文件名 tar -xvzf file.tar.gz -C /path/to/outdir # 指定目录 压缩 # tar -cvzf archive.tar.gz dir1 file2 … Curl # curl -L -O \u0026#34;https://example.com/path/file.bin\u0026#34; -L：遇到 30x 重定向时跟着跳转（很多下载链接会 302）。 -o 保存文件名：把内容写进文件，而不是 stdout；文件名自己起，后缀保持 .bin、.zip、.jpg 等原始格式即可。 -C -（可选）：断点续传，网络中断后重新跑同一条命令可接着下。 -O：如果 URL 里包含文件名则自动识别，如果没包含会 404。 Top # 查看系统占用的命令。可以用来查看一下当前的命令有没有跑完。\nEcho # 别的没啥说的，今天发现 echo -e \u0026quot;\\a\\a\u0026quot; 可以让电脑蜂鸣两声（但在 Alacritty ssh 上无效）。\n\\a 是 ASCII 的 BEL 字符（0x07）。 -e 让 echo 解释转义序列；两个 \\a 就是连续两次蜂鸣信号。 ","date":"5 August 2025","externalUrl":null,"permalink":"/posts/ai/engineer/%E5%9C%A8%E4%BA%91gpu%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%9E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E5%BF%85%E4%BC%9A%E7%9A%84%E5%87%A0%E4%B8%AA%E5%91%BD%E4%BB%A4/","section":"Posts","summary":"","title":"在云GPU环境下搞深度学习训练，必会的几个命令","type":"posts"},{"content":" Score Matching # 我觉得 Score matching by Andy Jones 说得挺清楚了，所以这一篇笔记主要是回忆性质，记下几个 key point 和 key idea。\nScore-matching 要解决的问题是？ 如何理解 Score 函数？Score 函数是如何解决 normalizing constant 的问题的？ 如何衡量两个分布的梯度的差距？为什么用 Fisher 散度？ 为什么涉及到真实分布 $p_d(x)$ 的就不好求？ 🙋什么是 Score Matching？ # Score Matching Score Matching 是一种用于拟合统计模型的方法，特别适用于处理不可计算归一化常数（intractable normalizing constants）的模型。在机器学习中，当模型的似然函数复杂且难以归一化时（例如在能量基模型（EBMs）、生成对抗网络（GANs）或变分自编码器（VAEs 中），Score Matching 提供了一种绕过归一化常数的方法来优化模型参数。\n假设我们有一组观测数据 $x_1, \\dots, x_n$，这些数据服从未知的真实分布 $p_d(x)$。我们希望用一个参数化的模型 model 分布 $p_m(x; \\theta)$ 来近似 distribution $p_d(x)$，其中 $\\theta$ 是模型参数。目标是找到合适的 $\\theta$，使 $p_m(x; \\theta)$ 尽可能接近 $p_d(x)$。\n在最大似然估计（MLE）中，我们通常通过最大化数据的对数似然来优化 $\\theta$：\n$$ \\widehat{\\theta}_{MLE} = argmax_{\\theta} \\log p_m(x; \\theta). $$模型的概率密度函数通常可以写成未归一化的密度 $\\widetilde{p}(x; \\theta)$ 和归一化常数 $Z_\\theta$ 的形式：\n$$ p_m(x; \\theta) = \\frac{\\widetilde{p}(x; \\theta)}{Z_\\theta}, \\quad Z_\\theta = \\int_{\\mathcal{X}} \\widetilde{p}(x; \\theta) , dx. $$这里的 $Z_\\theta$ 是一个 normalizing constant，通常在复杂模型中难以计算（即不可解，intractable）。Score Matching 的核心思想是通过避免直接计算 $Z_\\theta$，来解决这一问题。\n我觉得这里完全有必要说一下：难在哪儿？为什么不能求出 $p_d(x)$？ Note 因为真实分布只有样本，没有分布，我们可以假设模型分布 $p_m$ 为高斯啦学生分布啦等等，但是真实分布通常有可能是高斯混合模型，很难搞。所以我们得想办法消除 $p_d(x)$ 这一项。在后文中还会出现一次 $p_{m}$ 项，留意之。\n💡Score Matching 的核心思想 # 其实我们如果注意到，对上面的 model 分布 p 对 x 求梯度，那么 Z 这一项就会消失（因为参数里没有 x）。\n在 Score Matching 中，==score 函数==是指对数似然函数关于数据 $x$ 的梯度：\n$$ s_{\\theta}=\\nabla_x \\log p_m(x; \\theta). $$将其展开，我们可以看到归一化常数的优势：\n$$ \\nabla_x \\log p_m(x; \\theta) = \\nabla_x \\log \\widetilde{p}_m(x; \\theta) - \\nabla_x \\log Z_\\theta. $$由于 $Z_\\theta$ 不依赖于 $x$，其梯度 $\\nabla_x \\log Z_\\theta = 0$，因此：\n$$ \\nabla_x \\log p_m(x; \\theta) = \\nabla_x \\log \\widetilde{p}_m(x; \\theta). $$Normalizing constant Z 消失了！那么 Score matching 的核心思想变呼之欲出：如果建模分布 $p_m$ 与原始分布 $p_d$ 相似，那么他们的梯度也应该相似。最后顶多差一个偏移而已。那么我们不求（或者以后再想办法求）$p(x)$，而是去求 $p_{x}$ 对数的梯度（Score-function），让 score-function 尽可能逼近真实数据的对数梯度。这个就是 Score-matching 方法。\n🎯Score Matching 的目标 # Score Matching 的目标是最小化模型分布的 score 函数与真实数据分布的 score 函数之间的 Fisher 散度（Fisher Divergence）：\n$$ \\widehat{\\theta}_{SM} = argmin_{\\theta} D_F(p_d, p_m) = argmin_{\\theta} \\frac{1}{2} \\mathbb{E}_{p_d} \\left[ | \\nabla_x \\log p_d(x) - \\nabla_x \\log p_m(x; \\theta) |_2^2 \\right]. $$ 这是因为只有 fisher 散度可以跟梯度联系起来，而 KL 散度是做不到的。 但到了这里还是有 $p_d(x)$ 项，还是不好求。所以自然地想到下一步要怎么做。\n↻绕过归一化常数和数据分布 # 展开 Fisher 散度： $$ \\frac{1}{2} | \\nabla_x \\log p_d(x) - \\nabla_x \\log p_m(x; \\theta) |_2^2 = \\frac{1}{2} (\\nabla_x \\log p_d(x))^2 - \\nabla_x \\log p_m(x; \\theta) \\nabla_x \\log p_d(x) + \\frac{1}{2} (\\nabla_x \\log p_m(x; \\theta))^2. $$ 第一项 $\\frac{1}{2} (\\nabla_x \\log p_d(x))^2$ 是常数项，不依赖 $\\theta$，不影响优化 $\\theta$，可以忽略。 第三项 $\\frac{1}{2} (\\nabla_x \\log p_m(x; \\theta))^2$ 可以通过数据样本直接估计，因为它不依赖 $p_d(x)$。 处理交叉项： $$ \\mathbb{E}_{p_d} \\left[ -\\nabla_x \\log p_m(x; \\theta) \\nabla_x \\log p_d(x) \\right] = -\\int_{-\\infty}^{\\infty} \\nabla_x \\log p_m(x; \\theta) \\nabla_x \\log p_d(x) p_d(x) , dx. $$通过分部积分法（integration by parts）（这里很巧妙但是我不细写了），假设边界项在无穷远处消失（即 $p_d(x) \\nabla_x \\log p_m(x; \\theta) \\to 0$ 当 $|x|_2 \\to \\infty$），我们可以将交叉项转化为：\n$$ \\int_{-\\infty}^{\\infty} \\nabla_x^2 \\log p_m(x; \\theta) p_d(x) dx. $$那么问题来了：==为什么会假设边界项消失？==\n边界项消失 边界项 $p_d(x) \\nabla_x \\log p_m(x; \\theta) \\to 0$ 当 $|x|_2 \\to \\infty$ 是一个正则化条件，确保分部积分成立。直观上：\n数据分布 $p_d(x)$ 通常在无穷远处迅速衰减到零，因为实际数据集中在有限区域，尾部概率很小。例如高斯分布 $p_d(x) \\propto \\exp(-x^2 / (2\\sigma_d^2))$ 的尾部以指数速度衰减。 模型 score 函数 $\\nabla_x \\log p_m(x; \\theta)$ 通常增长较慢（例如高斯模型中为线性增长）。因此，乘积 $p_d(x) \\nabla_x \\log p_m(x; \\theta)$ 在无穷远处趋于零，因为数据分布的尾部衰减比 score 函数的增长快。 这就像在积分中，尾部贡献变得微不足道，因为数据分布在无穷远处几乎没有概率质量。 在高斯分布的例子中：\n$$ p_d(x)\\nabla_x \\log p_m(x;\\mu,\\sigma^2) \\sim \\exp\\!\\left(-\\frac{x^{2}}{2\\sigma_d^{2}}\\right)\\cdot\\frac{\\mu-x}{\\sigma^{2}} \\to 0 $$因为指数衰减比线性增长快得多。\n因为 $\\nabla_x^2 \\log p_m(x; \\theta) p_d(x) dx$ 在大于等于 2 阶情况下的其实是个 Hessian 矩阵，对角线的元素才是对 x 求二阶导，所以写成 tr 的形式 $$ \\int_{-\\infty}^{\\infty} \\nabla_x^2 \\log p_m(x; \\theta) p_d(x) dx = \\mathbb{E}_{p_d} \\left[ \\text{tr} \\left( \\nabla_x^2 \\log p_m(x; \\theta) \\right) \\right] $$所以损失函数：\n$$ D_F(p_d, p_m) \\propto L(\\theta) = \\mathbb{E}_{p_d} \\left[ \\text{tr} \\left( \\nabla_x^2 \\log p_m(x; \\theta) \\right) + \\frac{1}{2} ||\\nabla_x \\log p_m(x; \\theta)||_2^2 \\right]. $$使用数据样本 $x_1, \\dots, x_n$，目标函数可近似为：\n$$ L(\\theta) \\approx \\frac{1}{n} \\sum_{i=1}^n \\left[ \\text{tr} \\left( \\nabla_x^2 \\log p_m(x_i; \\theta) \\right) + \\frac{1}{2} |\\nabla_x \\log p_m(x_i; \\theta)|_2^2 \\right]. $$这个目标函数完全不依赖归一化常数 $Z_\\theta$ 和真实分布 $p_d(x)$，只需要模型的未归一化密度 $\\widetilde{p}_m(x; \\theta)$ 和数据样本即可。\nHyvärinen 证明，如果真实分布 $p_d(x) = p_m(x; \\theta^\\star)$ 属于模型族，则优化 $L(\\theta)$ 可找到最优参数 $\\theta^\\star$。\n只要真实分布 恰好能被模型族中的某个参数 $\\theta^\\star$ 表示出来，那么最小化得分匹配目标 $L(θ)$ 就一定能把这个 $\\theta^\\star$ 找出来。\n🤔目标函数的直观理解 # 目标函数由两部分组成：\nNorm 项：$\\frac{1}{2} |\\nabla_x \\log p_m(x_i; \\theta)|_2^2$ 表示模型 score 函数的大小。 当数据点 $x_i$ 被模型很好地解释时（即位于似然的高概率区域），score 函数的值较小（似然变化平缓）。 直观上，这个项希望模型的似然函数在数据点附近平滑。 Hessian 项：$\\text{tr} \\left( \\nabla_x^2 \\log p_m(x_i; \\theta) \\right)$ 表示对数似然的二阶导数（Hessian 矩阵的迹）。 如果数据点位于“尖锐”的局部极小值处（Hessian 迹为负且绝对值较大），说明模型对数据的解释更“独特”。 直观上，这个项倾向于选择更“尖锐”的似然函数，避免过于平坦的似然（平坦的似然意味着多种参数值都能解释数据）。 这两个项相互平衡：Norm 项倾向于平滑的似然，Hessian 项倾向于尖锐的似然，优化 $L(\\theta)$ 找到既能解释数据又具有适当曲率的模型。\n📖方法论 # 所以总结一下，最后其实很简单——甚至损失函数只和 $p_m$ 有关。假设了 $p_m$ 的分布之后：\n先求其关于训练数据的一阶导数的平方（Norm 项） 再在 Norm 项算式的基础上求其二阶导（Hessian 项） 相加，求 $argmin_{\\theta}$，就完事儿了。Score matching by Andy Jones 的最后举了一个高斯的例子，很不错。 📚 延伸阅读 # Hyvärinen (2005): 原始论文，提出得分匹配。 Song \u0026amp; Ermon (2019): 使用得分匹配进行生成建模。 Sliced Score Matching (2020): 可扩展版本，适用于高维数据。 ","date":"21 July 2025","externalUrl":null,"permalink":"/posts/ai/aigc/cornerstone/score-matching-%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%8E%A8%E5%AF%BC/","section":"Posts","summary":"","title":"Score matching 的基本推导","type":"posts"},{"content":" About Me # Hi, this is Niyuta ✨\n🎓 Education # M.S. in Deep Learning @ Beihang University (BUAA) B.S. in Computer Science @ Beihang University (BUAA) 💻 Technical Stack # Languages \u0026amp; Tools:\nProficient: Python, Rust (perhaps) Familiar: Java, C#, JavaScript/TypeScript, C/C++, SQL Misc: Git, Docker, Linux, Qemu, Cloud, React, Vue, etc. 🔭 Current Interests # Machine Learning \u0026amp; Deep Learning Systems (Python 🐍) High-performance Computing (Rust 🦀) Operating Systems (Rust 🦀) Web Development (React \u0026amp; Vue \u0026amp; Go 🎉) 🎸 Beyond Code # Guitarist crafting riffs when not debugging Anime cosplayer bringing 2D to 3D Eternal student of math \u0026amp; physics ","date":"28 February 2019","externalUrl":null,"permalink":"/page/about/","section":"Pages","summary":"","title":"About","type":"page"},{"content":"","date":"28 February 2019","externalUrl":null,"permalink":"/page/","section":"Pages","summary":"","title":"Pages","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]